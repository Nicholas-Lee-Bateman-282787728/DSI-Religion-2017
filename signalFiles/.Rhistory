"broccoli", "apples", "bread", "salmon"), nutrition = c(0, 15, 1, 20, 30,
17, 0, 50), calories = c(700, 450, 300, 200, 50, 75, 200, 200), cost = c(5, 8, 3, 4, 2, 3, 2, 10))
dataset
## This is the budget for my shopping trip
budget <- 20
## For this problem, chromosomes represent which items are to be taken and which
## are to be left behind.  "1" indicates that the object should be packed and
## taken, while "0" indicates that the corresponding object should be left behind.
## Here is an example chromosome:
chromosome = c(1, 0, 0, 1, 1, 0, 0, 1)
dataset[chromosome == 1, ]
## We can check the sum of the calories and nutrition for this particular chromosome:
cat(chromosome %*% dataset$cost)
dataset$calories<-(dataset$calories - mean(dataset$calories)) / sd(dataset$calories)
dataset$calories<-(dataset$calories - mean(dataset$calories)) / sd(dataset$calories)
dataset$nutrition<- (dataset$nutrition - mean(dataset$nutrition))/sd(dataset$nutrition)
dataset <- data.frame(item = c("pizza", "steak", "chips", "chicken",
"broccoli", "apples", "bread", "salmon"), nutrition = c(0, 15, 1, 20, 30,
17, 0, 50), calories = c(700, 450, 300, 200, 50, 75, 200, 200), cost = c(5, 8, 3, 4, 2, 3, 2, 10))
dataset
## This is the budget for my shopping trip
budget <- 20
#Scale Data
dataset$calories<-(dataset$calories - mean(dataset$calories)) / sd(dataset$calories)
dataset$nutrition<- (dataset$nutrition - mean(dataset$nutrition))/sd(dataset$nutrition)
## For this problem, chromosomes represent which items are to be taken and which
evalFunc <- function(x) {
current_solution_cost <- x %*% dataset$cost
current_solution_nutrition <- x %*% dataset$nutrition
current_solution_calories<- x %*% dataset$calories
if (current_solution_cost > budget)
return(0) else return(-current_solution_calories-current_solution_nutrition )
}
## Next, the number of iterations is chosen and the model is designed and run.
iter = 100
## To run the model, we give the number of genes in the chromosome (size),  the population
## size, the number of generations or iterations, the mutation rate, whether or not we
## want to use elitism in the model and we indicate the fitness function to be used.
GAmodel <- rbga.bin(size = 8, popSize = 200, iters = iter, mutationChance = 0.01,
elitism = T, evalFunc = evalFunc, verbose = TRUE)
cat(genalg:::summary.rbga(GAmodel))
solution = c(1, 0, 0, 0 ,0 , 0, 0, 1)
dataset[solution == 1, ]
install.packages("DMwR")
library(DMwR)
data(iris)
data <- iris[, c(1, 2, 5)]
data$Species <- factor(ifelse(data$Species == "setosa","rare","common"))
table(data$Species)
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100)
table(newData$Species)
par(mfrow = c(1, 2))
plot(data[, 1], data[, 2], pch = 19 + as.integer(data[, 3]),
main = "Original Data")
plot(newData[, 1], newData[, 2], pch = 19 + as.integer(newData[,3]),
main = "SMOTE'd Data")
classTree <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100,
learner='rpartXse',se=0.5)
classTree
rpartXse(Species ~ .,data,se=0.5)
View(bn_df)
library(DMwR)
install.packages('SMOTE')
library(randomForest)
library(caret)
library(randomForest)
setwd('/Users/meganstiles/Desktop/github/DSI-Religion-2017/signalFiles/')
list.files()
signals<- read.csv('SingleDocSignals.csv')
signals<- signals[,-1]
#Set Rank as Factor Variable
signals$rank<- as.factor(signals$rank)
set.seed(21)
folds<-createFolds(signals$rank, k=10, list = TRUE, returnTrain = FALSE)
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = signals[-test.indices,]
test = signals[test.indices,]
train$rank = factor(train$rank)
test$rank = factor(test$rank)
#train Model
model <- randomForest(rank ~., data = train)
#Make predictions based on model for testing set
predictions<- predict(model, newdata = test)
#Calculate Accuracy
for (j in 1: length(test)) {
diff<- abs(as.numeric(test$rank[j]) - as.numeric(predictions[j]))
difference[j]<- diff
}
#Calculate Mean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
raw_accuracy
mean(raw_accuracy)
View(signals)
signals<- read.csv('SingleDocSignals.csv')
#Drop Unneeded Columns
signals<- signals[,-1]
#Set Rank as Factor Variable
signals$rank<- as.factor(signals$rank)
set.seed(21)
folds<-createFolds(signals$rank, k=10, list = TRUE, returnTrain = FALSE)
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = signals[-test.indices,]
test = signals[test.indices,]
train$rank = factor(train$rank)
test$rank = factor(test$rank)
#train Model
model <- randomForest(rank ~-groupId., data = train)
#Make predictions based on model for testing set
predictions<- predict(model, newdata = test)
#Calculate Accuracy
for (j in 1: length(test)) {
diff<- abs(as.numeric(test$rank[j]) - as.numeric(predictions[j]))
difference[j]<- diff
}
#Calculate Mean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
raw_accuracy
mean(raw_accuracy)
signals<- read.csv('SingleDocSignals.csv')
#Drop Unneeded Columns
signals<- signals[,-1]
#Set Rank as Factor Variable
signals$rank<- as.factor(signals$rank)
set.seed(21)
folds<-createFolds(signals$rank, k=10, list = TRUE, returnTrain = FALSE)
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = signals[-test.indices,]
test = signals[test.indices,]
train$rank = factor(train$rank)
test$rank = factor(test$rank)
#train Model
model <- randomForest(rank~. -groupId, data = train)
#Make predictions based on model for testing set
predictions<- predict(model, newdata = test)
#Calculate Accuracy
for (j in 1: length(test)) {
diff<- abs(as.numeric(test$rank[j]) - as.numeric(predictions[j]))
difference[j]<- diff
}
#Calculate Mean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
raw_accuracy
mean(raw_accuracy)
setwd('/Users/meganstiles/Desktop/github/DSI-Religion-2017/signalFiles/')
#gradient Boosting
require(xgboost)
library(caret)
library(dplyr)
df_clean = read.csv('SingleDocSignals.csv')
#Reset Rank Levels, for xgboost in multiclass classification, the classes are (0, num_class) so we subtract one from rank
df_clean$rank<- df_clean$rank - 1
#Set Rank as Factor
df_clean$rank<- as.factor(df_clean$rank)
#Drop Unneeded Variables:
df_clean<- df_clean[,-c(1)]
#10 fold CV
#Set Seed
set.seed(21)
#Create Folds
folds<- createFolds(df_clean$rank, k=10, list = TRUE, returnTrain = FALSE)
param <- list("objective" = "multi:softprob",
"num_class" = 9,
'max_depth' = 5,
'eval_metric' = 'merror')
#initialzie empty vector to store accuracy
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = df_clean[-test.indices,]
test = df_clean[test.indices,]
#Convert to Matrix
train_X<-data.matrix(train[,c(1:18, 20)])
train_Y<- data.matrix(train$rank)
test_X<- data.matrix(test[,c(1:18, 20)])
test_Y = data.matrix(test$rank)
#train Model
model <- xgboost(param=param, data=train_X, label=train_Y, nrounds=5)
#Make predictions based on model for testing set
predictions<- predict(model, test_X)
# reshape it to a num_class-columns matrix
pred <- matrix(predictions, ncol=9, byrow=TRUE)
# convert the probabilities to softmax labels (we have to subtract  one)
pred_labels <- max.col(pred) - 1
#Find the difference between the predicted value and the actual value
for (j in 1: length(test_Y)) {
diff<- abs(as.numeric(test_Y[j]) - pred_labels[j])
difference[j]<- diff
}
#Calculate Aean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
Avg_Accuracy = mean(raw_accuracy)
Avg_Accuracy #MAE = 1.08393
df_clean = read.csv('binnedSignals1.csv')
View(signals)
df_clean$rank<- df_clean$rank - 1
df_clean$rank<- as.factor(df_clean$rank)
df_clean<- df_clean[,-c(1)]
#Set Seed
set.seed(21)
#Create Folds
folds<- createFolds(df_clean$rank, k=10, list = TRUE, returnTrain = FALSE)
param <- list("objective" = "multi:softprob",
"num_class" = 9,
'max_depth' = 5,
'eval_metric' = 'merror')
#initialzie empty vector to store accuracy
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = df_clean[-test.indices,]
test = df_clean[test.indices,]
#Convert to Matrix
train_X<-data.matrix(train[,c(2:17)])
train_Y<- data.matrix(train$rank)
test_X<- data.matrix(test[,c(2:17, 20)])
test_Y = data.matrix(test$rank)
#train Model
model <- xgboost(param=param, data=train_X, label=train_Y, nrounds=5)
#Make predictions based on model for testing set
predictions<- predict(model, test_X)
# reshape it to a num_class-columns matrix
pred <- matrix(predictions, ncol=9, byrow=TRUE)
# convert the probabilities to softmax labels (we have to subtract  one)
pred_labels <- max.col(pred) - 1
#Find the difference between the predicted value and the actual value
for (j in 1: length(test_Y)) {
diff<- abs(as.numeric(test_Y[j]) - pred_labels[j])
difference[j]<- diff
}
#Calculate Aean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
Avg_Accuracy = mean(raw_accuracy)
Avg_Accuracy
#Set Seed
set.seed(21)
#Create Folds
folds<- createFolds(df_clean$rank, k=10, list = TRUE, returnTrain = FALSE)
param <- list("objective" = "multi:softprob",
"num_class" = 9,
'max_depth' = 5,
'eval_metric' = 'merror')
#initialzie empty vector to store accuracy
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = df_clean[-test.indices,]
test = df_clean[test.indices,]
#Convert to Matrix
train_X<-data.matrix(train[,c(2:17)])
train_Y<- data.matrix(train$rank)
test_X<- data.matrix(test[,c(2:17)])
test_Y = data.matrix(test$rank)
#train Model
model <- xgboost(param=param, data=train_X, label=train_Y, nrounds=5)
#Make predictions based on model for testing set
predictions<- predict(model, test_X)
# reshape it to a num_class-columns matrix
pred <- matrix(predictions, ncol=9, byrow=TRUE)
# convert the probabilities to softmax labels (we have to subtract  one)
pred_labels <- max.col(pred) - 1
#Find the difference between the predicted value and the actual value
for (j in 1: length(test_Y)) {
diff<- abs(as.numeric(test_Y[j]) - pred_labels[j])
difference[j]<- diff
}
#Calculate Aean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
Avg_Accuracy = mean(raw_accuracy)
Avg_Accuracy
library(kernlab)
setwd('/Users/meganstiles/Desktop/github/DSI-Religion-2017/signalFiles/')
list.files()
signals<- read.csv('binnedSignals1.csv')
#Drop Unneeded Columns
signals<- signals[,-1]
#Set Rank as Factor Variable
signals$rank<- as.factor(signals$rank)
#Set Seed
set.seed(21)
# 10-Fold CV
folds<-createFolds(signals$rank, k=10, list = TRUE, returnTrain = FALSE)
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = signals[-test.indices,]
test = signals[test.indices,]
train$rank = factor(train$rank)
test$rank = factor(test$rank)
#train Model
model <- ksvm(rank ~. -groupId, data = train, type="C-svc", kernel="vanilladot", C=100)
#Make predictions based on model for testing set
predictions<- predict(model, newdata = test)
#Calculate Accuracy
for (j in 1: length(test)) {
diff<- abs(as.numeric(test$rank[j]) - as.numeric(predictions[j]))
difference[j]<- diff
}
#Calculate Mean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
raw_accuracy
mean(raw_accuracy)
library(kernlab)
setwd('/Users/meganstiles/Desktop/github/DSI-Religion-2017/signalFiles/')
list.files()
signals<- read.csv('SingleDocSignals.csv')
#Drop Unneeded Columns
signals<- signals[,-1]
#Set Rank as Factor Variable
signals$rank<- as.factor(signals$rank)
#Set Seed
set.seed(21)
# 10-Fold CV
folds<-createFolds(signals$rank, k=10, list = TRUE, returnTrain = FALSE)
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = signals[-test.indices,]
test = signals[test.indices,]
train$rank = factor(train$rank)
test$rank = factor(test$rank)
#train Model
model <- ksvm(rank ~. -groupId, data = train, type="C-svc", kernel="vanilladot", C=100)
#Make predictions based on model for testing set
predictions<- predict(model, newdata = test)
#Calculate Accuracy
for (j in 1: length(test)) {
diff<- abs(as.numeric(test$rank[j]) - as.numeric(predictions[j]))
difference[j]<- diff
}
#Calculate Mean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
raw_accuracy
mean(raw_accuracy)
signals<- read.csv('binnedSignals1.csv')
signals<- signals[,-1]
signals$rank<- as.factor(signals$rank)
set.seed(21)
folds<-createFolds(signals$rank, k=10, list = TRUE, returnTrain = FALSE)
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = signals[-test.indices,]
test = signals[test.indices,]
train$rank = factor(train$rank)
test$rank = factor(test$rank)
#train Model
model <- ksvm(rank ~. -groupId, data = train, type="C-svc", kernel="vanilladot", C=100)
#Make predictions based on model for testing set
predictions<- predict(model, newdata = test)
#Calculate Accuracy
for (j in 1: length(test)) {
diff<- abs(as.numeric(test$rank[j]) - as.numeric(predictions[j]))
difference[j]<- diff
}
#Calculate Mean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
raw_accuracy
mean(raw_accuracy)
setwd('/Users/meganstiles/Desktop/github/DSI-Religion-2017/signalFiles/')
list.files()
signals<- read.csv('SingleDocSignals.csv')
#Drop Unneeded Columns
signals<- signals[,-1]
#Set Rank as Factor Variable
signals$rank<- as.factor(signals$rank)
set.seed(21)
folds<-createFolds(signals$rank, k=10, list = TRUE, returnTrain = FALSE)
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = signals[-test.indices,]
test = signals[test.indices,]
train$rank = factor(train$rank)
test$rank = factor(test$rank)
#train Model
model <- randomForest(rank~., data = train)
#Make predictions based on model for testing set
predictions<- predict(model, newdata = test)
#Calculate Accuracy
for (j in 1: length(test)) {
diff<- abs(as.numeric(test$rank[j]) - as.numeric(predictions[j]))
difference[j]<- diff
}
#Calculate Mean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
raw_accuracy
mean(raw_accuracy)
library(caret)
library(randomForest)
setwd('/Users/meganstiles/Desktop/github/DSI-Religion-2017/signalFiles/')
list.files()
signals<- read.csv('binnedSignals1.csv')
#Drop Unneeded Columns
signals<- signals[,-1]
#Set Rank as Factor Variable
signals$rank<- as.factor(signals$rank)
set.seed(21)
folds<-createFolds(signals$rank, k=10, list = TRUE, returnTrain = FALSE)
raw_accuracy<- vector()
difference<- vector()
i=0
j=0
for (i in 1:10) {
#Create testing indicies based on folds
test.indices<- folds[[i]]
#Create training and testing sets
train = signals[-test.indices,]
test = signals[test.indices,]
train$rank = factor(train$rank)
test$rank = factor(test$rank)
#train Model
model <- randomForest(rank ~. -groupId, data = train)
#Make predictions based on model for testing set
predictions<- predict(model, newdata = test)
#Calculate Accuracy
for (j in 1: length(test)) {
diff<- abs(as.numeric(test$rank[j]) - as.numeric(predictions[j]))
difference[j]<- diff
}
#Calculate Mean Absolute Error
accuracy<-mean(difference)
#Store accuracy for each run in vector
raw_accuracy[i]= accuracy
}
raw_accuracy
mean(raw_accuracy)
