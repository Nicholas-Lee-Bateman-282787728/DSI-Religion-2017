{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Seth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /Users/Seth/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Seth/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "2016-10-11 12:26:31.202737\n",
      "finished loading packages after 0.00165414810181 seconds\n"
     ]
    }
   ],
   "source": [
    "# %load masterOOScript.py\n",
    "\"\"\"\n",
    "Created on Tue Jul 19 16:14:43 2016\n",
    "\n",
    "@author: nmvenuti\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun  2 15:23:11 2016\n",
    "\n",
    "@author: nmvenuti\n",
    "\"\"\"\n",
    "import time\n",
    "start=time.time()\n",
    "import sys, os\n",
    "\n",
    "os.chdir('/Users/Seth/Documents/DSI/Capstone/DSI-Religion-2017')\n",
    "#from joblib import Parallel, delayed\n",
    "#import multiprocessing as mp\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "sys.path.append('./prototype_python/')\n",
    "import lingualNOiGRAPH as la\n",
    "#import lingual as la\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "end=time.time()\n",
    "#sys.stdout = open(\"output.txt\", \"a\")\n",
    "print(str(datetime.now()))\n",
    "print('finished loading packages after '+str(end-start)+' seconds')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "\n",
    "######\n",
    "import random\n",
    "import string\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocoWindow 3\n",
      "cvWindow 3\n",
      "netAngle 30\n",
      "startCount 0\n",
      "finished loading packages after 9.13473200798 seconds\n",
      "finished randomly creating subgroups 9.17124009132 seconds\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './data_dsicap/Shepherd/raw/has-your-flight-been-delayed-steve-shepherd-sermon-on-doing-gods-will-33649.asp.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3bfde73d34c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mrunMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroupList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrossValidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroupSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargetWordCount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstartCount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcocoWindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvdInt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcvWindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetAngle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimCount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mendTimeTotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3bfde73d34c5>\u001b[0m in \u001b[0;36mrunMaster\u001b[0;34m(rawPath, groupList, crossValidate, groupSize, targetWordCount, startCount, cocoWindow, svdInt, cvWindow, netAngle, simCount)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m#Run calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mmasterOutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtextAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparamList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;31m#Create output file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutputDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasterOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'groupId'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'files'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'timeRun'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'perPos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'perNeg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'perPosDoc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'perNegDoc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'judgementCount'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'judgementFrac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'avgSD'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'avgEVC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3bfde73d34c5>\u001b[0m in \u001b[0;36mtextAnalysis\u001b[0;34m(paramList)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#Create lingual object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloTest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlingualObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubFileList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#Get coco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Seth/Documents/DSI/Capstone/DSI-Religion-2017/prototype_python/lingualNOiGRAPH.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fileList, useStem, useStopwords)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m#Extract raw text and update for encoding issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mrawData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mtextList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mtokenList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './data_dsicap/Shepherd/raw/has-your-flight-been-delayed-steve-shepherd-sermon-on-doing-gods-will-33649.asp.txt'"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "#####Define Functions#####\n",
    "##########################\n",
    "def textAnalysis(paramList):\n",
    "    startTime=time.time()\n",
    "    groupId=paramList[0]\n",
    "    fileList=paramList[1]\n",
    "    targetWordCount=paramList[2]\n",
    "    cocoWindow=paramList[3]\n",
    "    svdInt=paramList[4]\n",
    "    cvWindow=paramList[5]\n",
    "    simCount=paramList[6]\n",
    "    startCount=paramList[7]\n",
    "    netAngle=paramList[8]    \n",
    "    \n",
    "    #Get list of subfiles\n",
    "    subFileList=[x[1] for x in fileList if x[0]==groupId[0] and x[2]==groupId[1]]\n",
    "    \n",
    "    #Create lingual object\n",
    "    loTest=la.lingualObject(subFileList)\n",
    "\n",
    "    #Get coco\n",
    "    loTest.getCoco(cocoWindow)\n",
    "    \n",
    "    #Get DSM\n",
    "    loTest.getDSM(svdInt)\n",
    "    \n",
    "    #Set keywords\n",
    "    loTest.setKeywords('adjAdv',targetWordCount,startCount)\n",
    "\n",
    "    #######################            \n",
    "    ###Semantic analysis###\n",
    "    #######################\n",
    "    \n",
    "    #Get context vectors\n",
    "    loTest.getContextVectors(cvWindow)\n",
    "    \n",
    "    #Get average semantic density\n",
    "    avgSD=np.mean([x[1] for x in loTest.getSD(simCount)])\n",
    "    \n",
    "    ########################################\n",
    "    ###POS Tagging and Judgement Analysis###\n",
    "    ########################################\n",
    "    judgementAvg=list(np.mean(np.array([[x[1],x[2]] for x in loTest.getJudgements()]),axis=0))\n",
    "    \n",
    "    ########################\n",
    "    ###Sentiment Analysis###\n",
    "    ########################\n",
    "\n",
    "    sentimentList=loTest.sentimentLookup()\n",
    "    \n",
    "    ############################\n",
    "    ###Network Quantification###\n",
    "    ############################\n",
    "    #loTest.setNetwork(netAngle)\n",
    "    \n",
    "    #avgEVC=loTest.evc()\n",
    "    \n",
    "    endTime=time.time()\n",
    "    timeRun=endTime-startTime\n",
    "    print('finished running'+'_'.join(groupId)+' in '+str(end-start)+' seconds')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    #Append outputs to masterOutput\n",
    "    return(['_'.join(groupId)]+[len(subFileList),timeRun]+sentimentList+judgementAvg+[avgSD]+[avgEVC])   \n",
    "\n",
    "def runMaster(rawPath,groupList,crossValidate,groupSize,targetWordCount,startCount,cocoWindow,svdInt,cvWindow,netAngle,simCount):\n",
    "    ###############################\n",
    "    #####Raw File List Extract#####\n",
    "    ###############################\n",
    "\n",
    "    rawFileList=[]\n",
    "    for groupId in groupList:\n",
    "        for dirpath, dirnames, filenames in os.walk(rawPath+groupId+'/raw'):\n",
    "            for filename in [f for f in filenames ]:\n",
    "                if '.txt' in filename:\n",
    "                    rawFileList.append([groupId,os.path.join(dirpath, filename)])\n",
    "\n",
    "    #Make output directory\n",
    "    runDirectory='./pythonOutput/TEST-coco_'+str(cocoWindow)+'_cv_'+str(cvWindow)+'_netAng_'+str(netAngle)+'_sc_'+str(startCount)+id_generator()\n",
    "    os.makedirs(runDirectory)\n",
    "    end=time.time()\n",
    "    print('finished loading packages after '+str(end-start)+' seconds')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    #Perform analysis for each fold in cross validation\n",
    "    for fold in range(crossValidate):                \n",
    "        ###############################                \n",
    "        #####Set up random binning#####\n",
    "        ###############################\n",
    "                        \n",
    "        fileDF=pd.read_csv('./data_dsicap/test_train/fileSplit_'+str(fold)+'.csv')\n",
    "        \n",
    "        fileList=fileDF.values.tolist()\n",
    "        #print(fileList) #########\n",
    "        fileList=[[fileList[i][1],fileList[i][2],fileList[i][3]] for i in range(len(fileList))]\n",
    "        #print(fileList) ############\n",
    "        \n",
    "        #Get set of subgroups\n",
    "        subgroupList=[ list(y) for y in set((x[0],x[2]) for x in fileList) ]\n",
    "        \n",
    "        #Make output directory\n",
    "        outputDirectory=runDirectory+'/run'+str(fold)\n",
    "        os.makedirs(outputDirectory)\n",
    "        \n",
    "        #Print file splits to runDirectory\n",
    "        fileDF.to_csv(outputDirectory+'/fileSplits.csv')\n",
    "\n",
    "        end=time.time()\n",
    "        print('finished randomly creating subgroups '+str(end-start)+' seconds')\n",
    "        sys.stdout.flush()        \n",
    "        \n",
    "        ################################\n",
    "        #####Perform group analysis#####\n",
    "        ################################\n",
    "        \n",
    "        #Create paramList\n",
    "        paramList=[[x,fileList,targetWordCount,cocoWindow,svdInt,cvWindow,simCount,startCount,netAngle] for x in subgroupList]\n",
    "        \n",
    "        #Run calculation \n",
    "        masterOutput=[textAnalysis(x) for x in paramList]  \n",
    "        #Create output file\n",
    "        outputDF=pd.DataFrame(masterOutput,columns=['groupId','files','timeRun','perPos','perNeg','perPosDoc','perNegDoc','judgementCount','judgementFrac','avgSD','avgEVC'])\n",
    "        outputDF.to_csv(outputDirectory+'/masterOutput.csv')\n",
    "\n",
    "#Set inital conditions and run\n",
    "if __name__ == '__main__':\n",
    "    startTimeTotal=time.time()\n",
    "    rawPath = './data_dsicap/'\n",
    "#    groupList=['DorothyDay','JohnPiper','MehrBaba','NaumanKhan','PastorAnderson',\n",
    "#       'Rabbinic','Shepherd','Unitarian','WBC']\n",
    "    groupList=['NaumanKhan']\n",
    "    \n",
    "#    cocoWindow=int(sys.argv[1])\n",
    "#    cvWindow=int(sys.argv[2])\n",
    "#    startCount=int(sys.argv[3])\n",
    "#    netAngle=int(sys.argv[4])\n",
    "    cocoWindow=3\n",
    "    cvWindow=3\n",
    "    startCount=0\n",
    "    netAngle=30\n",
    "    crossValidate=1\n",
    "    groupSize=10\n",
    "#    testSplit=.3\n",
    "    targetWordCount=10\n",
    "    svdInt=50\n",
    "    simCount=1000\n",
    "    print('cocoWindow '+str(cocoWindow))\n",
    "    sys.stdout.flush()\n",
    "    print('cvWindow '+str(cvWindow))\n",
    "    sys.stdout.flush()\n",
    "    print('netAngle '+str(netAngle))\n",
    "    sys.stdout.flush()\n",
    "    print('startCount '+str(startCount))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    runMaster(rawPath,groupList,crossValidate,groupSize,targetWordCount,startCount,cocoWindow,svdInt,cvWindow,netAngle,simCount)\n",
    "        \n",
    "    endTimeTotal=time.time()\n",
    "    print('finished entire run in :'+str((endTimeTotal-startTimeTotal)/60)+' minutes')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
