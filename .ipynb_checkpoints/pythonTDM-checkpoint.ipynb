{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "\n",
    "os.chdir('/Users/Seth/Documents/DSI/Capstone/2016-group/cloneOf2016Code')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('maxent_treebank_pos_tagger')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHERE I GOT STUFF\n",
    "\n",
    "#### the custom function\n",
    "http://slendermeans.org/ml4h-ch4.html\n",
    "\n",
    "#### the documentation for CountVectorizer\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sklearn_tdm_df(docs, **kwargs):\n",
    "    '''\n",
    "    Create a term-document matrix (TDM) in the form of a pandas DataFrame\n",
    "    Uses sklearn's CountVectorizer function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs: a sequence of documents (files, filenames, or the content) to be\n",
    "        included in the TDM. See the `input` argument to CountVectorizer.\n",
    "    **kwargs: keyword arguments for CountVectorizer options.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tdm_df: A pandas DataFrame with the term-document matrix. Columns are terms,\n",
    "        rows are documents.\n",
    "    '''\n",
    "    # Initialize the vectorizer and get term counts in each document.\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "    word_counts = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # .vocabulary_ is a Dict whose keys are the terms in the documents,\n",
    "    # and whose entries are the columns in the matrix returned by fit_transform()\n",
    "    vocab = vectorizer.vocabulary_\n",
    "\n",
    "    # Make a dictionary of Series for each term; convert to DataFrame\n",
    "    count_dict = {w: Series(word_counts.getcol(vocab[w]).data) for w in vocab}\n",
    "    tdm_df = DataFrame(count_dict).fillna(0)\n",
    "    return tdm_df\n",
    "\n",
    "# Call the function on e-mail messages. The token_pattern is set so that terms are only\n",
    "# words with two or more letters (no numbers or punctuation)\n",
    "message_tdm = sklearn_tdm_df(train_df['message'],\n",
    "                             stop_words = 'english',\n",
    "                             charset_error = 'ignore',\n",
    "                             token_pattern = '[a-zA-Z]{2,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
