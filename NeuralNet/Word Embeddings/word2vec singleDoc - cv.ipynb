{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings on single documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model variation is CNN-rand\n",
      "Loading data...\n",
      "Number of training documents: 311\n",
      "Vocabulary Size: 62558\n",
      "Fold:0, Missed 5 out of 31, Accuracy:0.838710\n",
      "Number of training documents: 310\n",
      "Vocabulary Size: 63468\n",
      "Fold:1, Missed 10 out of 32, Accuracy:0.687500\n",
      "Number of training documents: 307\n",
      "Vocabulary Size: 63117\n",
      "Fold:2, Missed 8 out of 35, Accuracy:0.771429\n",
      "Number of training documents: 306\n",
      "Vocabulary Size: 63955\n",
      "Fold:3, Missed 7 out of 36, Accuracy:0.805556\n",
      "Number of training documents: 306\n",
      "Vocabulary Size: 61887\n",
      "Fold:4, Missed 6 out of 36, Accuracy:0.833333\n",
      "Number of training documents: 308\n",
      "Vocabulary Size: 61878\n",
      "Fold:5, Missed 11 out of 34, Accuracy:0.676471\n",
      "Number of training documents: 306\n",
      "Vocabulary Size: 61501\n",
      "Fold:6, Missed 5 out of 36, Accuracy:0.861111\n",
      "Number of training documents: 307\n",
      "Vocabulary Size: 62113\n",
      "Fold:7, Missed 9 out of 35, Accuracy:0.742857\n",
      "Number of training documents: 307\n",
      "Vocabulary Size: 63112\n",
      "Fold:8, Missed 10 out of 35, Accuracy:0.714286\n",
      "Number of training documents: 310\n",
      "Vocabulary Size: 63149\n",
      "Fold:9, Missed 12 out of 32, Accuracy:0.625000\n",
      "[0.83870967741935487, 0.6875, 0.77142857142857146, 0.80555555555555558, 0.83333333333333337, 0.67647058823529416, 0.86111111111111116, 0.74285714285714288, 0.7142857142857143, 0.625]\n",
      "Average Accuracy=0.755625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D\n",
    "import ast\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from __future__ import print_function\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from functools import reduce\n",
    "import os\n",
    "from os.path import basename\n",
    "import csv\n",
    "\n",
    "sequence_length = 1000\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "#\n",
    "# Model Variations. See Kim Yoonâ€™s Convolutional Neural Networks for \n",
    "# Sentence Classification, Section 3 for detail.\n",
    "\n",
    "model_variation = 'CNN-google'  #  CNN-rand | CNN-google\n",
    "print('Model variation is %s' % model_variation)\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 3\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 100\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 5\n",
    "num_epochs = 5\n",
    "#val_split = 0.33\n",
    "\n",
    "# Word2Vec parameters, see train_word2vec\n",
    "min_word_count = 1  # Minimum word count                        \n",
    "context = 4        # Context window size    \n",
    "ACTION = \"train\"\n",
    "weights_file = \"weights_file\"\n",
    "TEXT_DATA_DIR='SingleDocSignals'\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "#\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "\n",
    "\n",
    "# Load 10-folds indicies produced from R (Avg_Accuracy:0.7222222)\n",
    "cv_files=[] \n",
    "with open('SingleDocSignals_R_cv_files','r') as intputFile:\n",
    "        reader=csv.reader(intputFile,delimiter=' ')\n",
    "        for row in reader:\n",
    "            cv_files.append(row)\n",
    "accuracy=[]\n",
    "allFiles=sorted(os.listdir(TEXT_DATA_DIR))\n",
    "for fold in range(0, len(cv_files)):\n",
    "    # load data\n",
    "    \n",
    "    train_sentences = []  # list of text articles\n",
    "    train_labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    train_labels = []  # list of label ids\n",
    "    test_sentences = []  # list of text articles\n",
    "    test_labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    test_labels = []  # list of label ids\n",
    "    for fname in allFiles:\n",
    "        fpath = os.path.join(TEXT_DATA_DIR, fname)\n",
    "        f = open(fpath)\n",
    "        if(fname in cv_files[fold]):\n",
    "            test_sentences.append(f.read())\n",
    "            test_labels_index[basename(fname)] = len(test_labels_index)\n",
    "            test_labels.append(-1)\n",
    "        else:\n",
    "            train_sentences.append(f.read())\n",
    "            train_labels_index[basename(fname)] = len(train_labels_index)\n",
    "            train_labels.append(-1)\n",
    "                \n",
    "        f.close()\n",
    "        \n",
    "\n",
    "    d = []\n",
    "    for i in train_sentences:\n",
    "        words2 = text_to_word_sequence(i, lower=True, split=\" \")\n",
    "        d.append(words2)\n",
    "\n",
    "    train_sentences = d\n",
    "    \n",
    "    train_vocab = sorted(reduce(lambda x, y: x | y, (set(i) for i in d)))\n",
    "    \n",
    "    d = []\n",
    "    for i in test_sentences:\n",
    "        words2 = text_to_word_sequence(i, lower=True, split=\" \")\n",
    "        d.append(words2)\n",
    "\n",
    "    test_sentences = d\n",
    "    \n",
    "    test_vocab = sorted(reduce(lambda x, y: x | y, (set(i) for i in d)))\n",
    "    \n",
    "    # Reserve 0 for masking via pad_sequences\n",
    "    train_vocab_size = len(train_vocab) + 1\n",
    "    train_word_idx = dict((c, i + 1) for i, c in enumerate(train_vocab))\n",
    "    \n",
    "    X = []\n",
    "    for i in train_sentences:\n",
    "        x = [train_word_idx[w] for w in i]\n",
    "        X.append(x)\n",
    "\n",
    "    X_train = pad_sequences(X,sequence_length)\n",
    "    \n",
    "    test_vocab_size = len(test_vocab) + 1\n",
    "    test_word_idx = dict((c, i + 1) for i, c in enumerate(test_vocab))\n",
    "    X = []\n",
    "    for i in test_sentences:\n",
    "        x = [test_word_idx[w] for w in i]\n",
    "        X.append(x)\n",
    "\n",
    "    X_test = pad_sequences(X,sequence_length)\n",
    "\n",
    "    #load labels\n",
    "    filePath='SingleDocSignals.csv'\n",
    "    with open(filePath,'r') as intputFile:\n",
    "            reader=csv.reader(intputFile,delimiter=',')\n",
    "            for fname,y in reader:\n",
    "                if((fname+\".txt\") in cv_files[fold]):\n",
    "                    test_labels[test_labels_index[fname+\".txt\"]]=int(y)\n",
    "                else:\n",
    "                    train_labels[train_labels_index[fname+\".txt\"]]=int(y)\n",
    "\n",
    "    Categories = train_labels\n",
    "    y = np.zeros(9)\n",
    "    outputs = list(set(Categories))\n",
    "    Y = []\n",
    "    for i in Categories:\n",
    "        y = np.zeros(9)\n",
    "        indexV = outputs.index(i)\n",
    "        y[indexV]=1\n",
    "        Y.append(y)\n",
    "    train_Y = np.asarray(Y)\n",
    "    \n",
    "    train_vocabulary= train_word_idx\n",
    "    train_vocabulary_inv = train_vocab \n",
    "    train_vocabulary_inv.append(\"</PAD>\")\n",
    "    \n",
    "    Categories = test_labels\n",
    "    y = np.zeros(9)\n",
    "    outputs = list(set(Categories))\n",
    "    Y = []\n",
    "    for i in Categories:\n",
    "        y = np.zeros(9)\n",
    "        indexV = outputs.index(i)\n",
    "        y[indexV]=1\n",
    "        Y.append(y)\n",
    "    test_Y = np.asarray(Y).argmax(axis=1)\n",
    "     \n",
    "    test_vocabulary= test_word_idx\n",
    "    test_vocabulary_inv = test_vocab \n",
    "    test_vocabulary_inv.append(\"</PAD>\")\n",
    "    \n",
    "    if model_variation=='CNN-google':\n",
    "        model_name='GoogleNews-vectors-negative300.bin'\n",
    "        embedding_model = Word2Vec.load_word2vec_format(model_name, binary=True)\n",
    "        embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n",
    "                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n",
    "                                                        for w in train_vocabulary_inv])]\n",
    "    elif model_variation=='CNN-rand':\n",
    "        embedding_weights = None\n",
    "    else:\n",
    "        raise ValueError('Unknown model variation')    \n",
    "     \n",
    "    \n",
    "    print(\"Number of training documents: {:d}\".format(len(X_train)))\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(train_vocabulary)))\n",
    "    \n",
    "    # find out how vocab is causing problems\n",
    "    \n",
    "    \n",
    "    # Building model\n",
    "    # ==================================================\n",
    "    #\n",
    "    # graph subnet with one input and one output,\n",
    "    # convolutional layers concateneted in parallel\n",
    "    graph_in = Input(shape=(sequence_length, embedding_dim))\n",
    "    convs = []\n",
    "    for fsz in filter_sizes:\n",
    "        conv = Convolution1D(nb_filter=num_filters,\n",
    "                             filter_length=fsz,\n",
    "                             border_mode='valid',\n",
    "                             activation='relu',\n",
    "                             subsample_length=1)(graph_in)\n",
    "        pool = MaxPooling1D(pool_length=2)(conv)\n",
    "        flatten = Flatten()(pool)\n",
    "        convs.append(flatten)\n",
    "    \n",
    "    if len(filter_sizes) > 1:\n",
    "        out = Merge(mode='concat')(convs)\n",
    "    else:\n",
    "        out = convs[0]\n",
    "    \n",
    "    graph = Model(input=graph_in, output=out)\n",
    "    # main sequential model\n",
    "    model = Sequential()\n",
    "    if not model_variation=='CNN-static':\n",
    "        model.add(Embedding(len(train_vocabulary_inv),embedding_dim, input_length=sequence_length,\n",
    "                            weights=embedding_weights))\n",
    "    \n",
    "    model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "    model.add(graph)\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(dropout_prob[1]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(9))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    \n",
    "    model.fit(X_train, train_Y, batch_size=batch_size,nb_epoch=num_epochs, verbose=0)\n",
    "   \n",
    "    # validate \n",
    "    test_count=len(X_test)\n",
    "    probs = model.predict(X_test.reshape(test_count, -1))\n",
    "    miss=np.sum( probs.argmax(axis=1)==test_Y)\n",
    "\n",
    "    acc=(test_count-miss)/test_count\n",
    "    accuracy.append(acc)\n",
    "    print(\"Fold:%i, Missed %i out of %i, Accuracy:%f\"%(fold,miss,test_count,acc))\n",
    "    \n",
    "print(accuracy)\n",
    "print(\"Average Accuracy=%f\"%np.average(accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
