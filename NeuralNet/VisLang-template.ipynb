{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "# This is referred above as f(u).\n",
    "class nn_MSECriterion:\n",
    "    def forward(self, predictions, labels):\n",
    "        return np.sum(np.square(predictions - labels))\n",
    "        \n",
    "    def backward(self, predictions, labels):\n",
    "        num_samples = labels.shape[0]\n",
    "        return num_samples * 2 * (predictions - labels) ### why num_samples * ... ?\n",
    "\n",
    "# This is referred above as g(v).\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # It is usually a good idea to use gv from the forward pass and not recompute it again here.\n",
    "        gv = 1 / (1 + np.exp(-x))  \n",
    "        return np.multiply(np.multiply(gv, (1 - gv)), gradOutput) ### what is gradOutput?\n",
    "\n",
    "# This is referred above as h(W, b)\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialized with random numbers from a gaussian N(0, 0.001)\n",
    "        self.weight = np.matlib.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.matlib.randn((1, output_dim)) * 0.01\n",
    "        self.gradWeight = np.zeros_like(self.weight)\n",
    "        self.gradBias = np.zeros_like(self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # dL/dw = dh/dw * dL/dv\n",
    "        self.gradWeight = np.dot(x.T, gradOutput)\n",
    "        # dL/db = dh/db * dL/dv\n",
    "        self.gradBias = np.copy(gradOutput)\n",
    "        # return dL/dx = dh/dx * dL/dv\n",
    "        return np.dot(gradOutput, self.weight.T)\n",
    "    \n",
    "    def getParameters(self):\n",
    "        params = [self.weight, self.bias]\n",
    "        gradParams = [self.gradWeight, self.gradBias]\n",
    "        return params, gradParams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learningRate = 0.1\n",
    "\n",
    "model = {}  \n",
    "model['linear1'] = nn_Linear(4, 5)\n",
    "model['linear2'] = nn_Linear(5, 3)\n",
    "model['sigmoid'] = nn_Sigmoid()\n",
    "model['loss'] = nn_MSECriterion()\n",
    "\n",
    "epochsToRun = 401\n",
    "for epoch in range(0, epochsToRun+1):\n",
    "    loss = 0\n",
    "    for i in range(0, dataset_size):\n",
    "        xi = x[i:i+1, :]\n",
    "        yi = y[i:i+1, :] \n",
    "\n",
    "        # Forward layer 1\n",
    "        a0_L1 = model['linear1'].forward(xi)\n",
    "        a1_L1 = model['sigmoid'].forward(a0_L1)\n",
    "        # Forward layer 2\n",
    "        a0_L2 = model['linear2'].forward(a1_L1)\n",
    "        a1_L2 = model['sigmoid'].forward(a0_L2)\n",
    "        #\n",
    "        loss += model['loss'].forward(a1_L2, yi)\n",
    "\n",
    "        # Backward layer 2\n",
    "        da1_L2 = model['loss'].backward(a1_L2, yi)\n",
    "        da0_L2 = model['sigmoid'].backward(a0_L2, da1_L2)\n",
    "        da1_L1 = model['linear2'].backward(a1_L1, da0_L2) # IS THIS RIGHT???\n",
    "\n",
    "        # Backward layer 1\n",
    "        #da1_L1 = model['loss'].backward(a1_L1, y2i) ### AND IS THIS WHAT GETS PASSED HERE?\n",
    "        da0_L1 = model['sigmoid'].backward(a0_L1, da1_L1)\n",
    "        model['linear1'].backward(xi, da0_L1)\n",
    "        \n",
    "        ##update layer 2\n",
    "        model['linear2'].weight = model['linear2'].weight - learningRate * model['linear2'].gradWeight\n",
    "        model['linear2'].bias = model['linear2'].bias - learningRate * model['linear2'].gradBias\n",
    "        #\n",
    "        ##update layer 1\n",
    "        model['linear1'].weight = model['linear1'].weight - learningRate * model['linear1'].gradWeight\n",
    "        model['linear1'].bias = model['linear1'].bias - learningRate * model['linear1'].gradBias\n",
    "          \n",
    "    if (epoch % 100 == 0) | (epoch == epochsToRun):\n",
    "        print('epoch[%d] = %.8f' % (epoch, loss / dataset_size))\n",
    "        #print('$$$ ' + 'weight = \\n' + str(model['linear'].weight))\n",
    "        #print('$$$ ' + 'bias = \\n' + str(model['linear'].bias))\n",
    "        print('************')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [visLang2]",
   "language": "python",
   "name": "Python [visLang2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
