{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_numpy_array(filePath, delimiter):\n",
    "    return np.genfromtxt(filePath, delimiter=delimiter, dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X is the TF matrix, Y is the binary response\n",
    "trainX = csv_to_numpy_array(\"trainXwiki.csv\", delimiter=\",\")\n",
    "trainY = csv_to_numpy_array(\"trainYwiki.csv\", delimiter=\",\")\n",
    "testX = csv_to_numpy_array(\"testXwiki.csv\", delimiter=\",\")\n",
    "testY = csv_to_numpy_array(\"testYwiki.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the index column from dataset\n",
    "trainX = trainX[:,1:14520]\n",
    "trainY = trainY[:,1:14520]\n",
    "testX = testX[:,1:14520]\n",
    "testY = testY[:,1:14520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove column headers\n",
    "trainX = trainX[1:14519]\n",
    "trainY = trainY[1:14519]\n",
    "testX = testX[1:14519]\n",
    "testY = testY[1:14519]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA SET PARAMETERS\n",
    "# get dimensions for different variables and placeholders:\n",
    "# numFeatures = the number of words extracted from each article\n",
    "numFeatures = trainX.shape[1]\n",
    "# numLabels = number of classes we're predicting (here it's 4 - Bahai, DorothyDay, IntegralYoga, and SeaShepherds)\n",
    "numLabels = trainY.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING SESSION PARAMETERS\n",
    "# number of times we iterate through training data\n",
    "# tensorboard shows that accuracy plateaus at ~25k epochs\n",
    "numEpochs = 27000\n",
    "# a smarter learning rate for gradientOptimizer\n",
    "learningRate = tf.train.exponential_decay(learning_rate = 0.0008,\n",
    "                                          global_step =1,\n",
    "                                          decay_steps = trainX.shape[0],\n",
    "                                          decay_rate = 0.95,\n",
    "                                          staircase = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = X-matrix/feature-matrix/data-matrix...It's a tensor to hold the article data. 'None' here \n",
    "# means that we can hold any number of articles\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "# yGold = Y-matrix/label-matrix/labels...This will be our correct answers matrix. Every row has\n",
    "# is binary, with 1 representing their group. 'None' here means that we can hold any number of emails\n",
    "yGold = tf.placeholder(tf.float32, [None, numLabels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Values are randomly sampled from a Gaussian with a standard deviation of:\n",
    "# sqrt(6/(numInputNodes + numOutputNodes + 1))\n",
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                        mean = 0,\n",
    "                                        stddev = (np.sqrt(6/numFeatures + numLabels + 1)),\n",
    "                                        name = \"weights\"))\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                     mean = 0,\n",
    "                                     stddev = (np.sqrt(6/numFeatures + numLabels + 1)),\n",
    "                                     name = \"bias\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### PREDICTION OPS ###\n",
    "######################\n",
    "# INITIALIZE our weights and biases\n",
    "init_OP = tf.initialize_all_variables()\n",
    "# PREDICTION ALGORITHM  i.e. FEEDFORWARD ALGORITHM\n",
    "apply_weights_OP = tf.matmul(X, weights, name = \"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name = \"add_bias\")\n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name = \"activation\") # sigmoid = activation function; output = 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### EVALUATION OP ###\n",
    "#####################\n",
    "# COST FUNCTION i.e. MEAN SQUARED ERROR\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-yGold, name = \"squared_error_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### OPTIMIZATION OP ###\n",
    "#######################\n",
    "# OPTIMIZATION ALGORITHM i.e. GRADIENT DESCENT\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Graph and Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.201681\n",
      "step 0, cost 266.612\n",
      "step 0, change in cost266.612\n",
      "step 10, training accuracy 0.201681\n",
      "step 10, cost 265.653\n",
      "step 10, change in cost0.958801\n",
      "step 20, training accuracy 0.201681\n",
      "step 20, cost 265.56\n",
      "step 20, change in cost0.0925903\n",
      "step 30, training accuracy 0.201681\n",
      "step 30, cost 265.464\n",
      "step 30, change in cost0.0965881\n",
      "step 40, training accuracy 0.201681\n",
      "step 40, cost 264.537\n",
      "step 40, change in cost0.927063\n",
      "step 50, training accuracy 0.201681\n",
      "step 50, cost 264.518\n",
      "step 50, change in cost0.0186462\n",
      "step 60, training accuracy 0.201681\n",
      "step 60, cost 264.504\n",
      "step 60, change in cost0.0135803\n",
      "step 70, training accuracy 0.201681\n",
      "step 70, cost 264.365\n",
      "step 70, change in cost0.139008\n",
      "step 80, training accuracy 0.201681\n",
      "step 80, cost 263.533\n",
      "step 80, change in cost0.832153\n",
      "step 90, training accuracy 0.201681\n",
      "step 90, cost 263.514\n",
      "step 90, change in cost0.0187683\n",
      "step 100, training accuracy 0.201681\n",
      "step 100, cost 263.511\n",
      "step 100, change in cost0.0038147\n",
      "step 110, training accuracy 0.201681\n",
      "step 110, cost 263.508\n",
      "step 110, change in cost0.00219727\n",
      "step 120, training accuracy 0.201681\n",
      "step 120, cost 263.507\n",
      "step 120, change in cost0.00158691\n",
      "step 130, training accuracy 0.201681\n",
      "step 130, cost 263.506\n",
      "step 130, change in cost0.0012207\n",
      "step 140, training accuracy 0.201681\n",
      "step 140, cost 263.505\n",
      "step 140, change in cost0.00106812\n",
      "step 150, training accuracy 0.201681\n",
      "step 150, cost 263.504\n",
      "step 150, change in cost0.000915527\n",
      "step 160, training accuracy 0.201681\n",
      "step 160, cost 263.503\n",
      "step 160, change in cost0.000823975\n",
      "step 170, training accuracy 0.201681\n",
      "step 170, cost 263.502\n",
      "step 170, change in cost0.000762939\n",
      "step 180, training accuracy 0.201681\n",
      "step 180, cost 263.501\n",
      "step 180, change in cost0.000701904\n",
      "step 190, training accuracy 0.201681\n",
      "step 190, cost 263.501\n",
      "step 190, change in cost0.000701904\n",
      "step 200, training accuracy 0.201681\n",
      "step 200, cost 263.5\n",
      "step 200, change in cost0.000701904\n",
      "step 210, training accuracy 0.201681\n",
      "step 210, cost 263.499\n",
      "step 210, change in cost0.000701904\n",
      "step 220, training accuracy 0.201681\n",
      "step 220, cost 263.499\n",
      "step 220, change in cost0.000732422\n",
      "step 230, training accuracy 0.201681\n",
      "step 230, cost 263.498\n",
      "step 230, change in cost0.000793457\n",
      "step 240, training accuracy 0.201681\n",
      "step 240, cost 263.497\n",
      "step 240, change in cost0.00088501\n",
      "step 250, training accuracy 0.201681\n",
      "step 250, cost 263.496\n",
      "step 250, change in cost0.00100708\n",
      "step 260, training accuracy 0.201681\n",
      "step 260, cost 263.495\n",
      "step 260, change in cost0.00119019\n",
      "step 270, training accuracy 0.201681\n",
      "step 270, cost 263.493\n",
      "step 270, change in cost0.00146484\n",
      "step 280, training accuracy 0.201681\n",
      "step 280, cost 263.491\n",
      "step 280, change in cost0.00189209\n",
      "step 290, training accuracy 0.201681\n",
      "step 290, cost 263.489\n",
      "step 290, change in cost0.00265503\n",
      "step 300, training accuracy 0.201681\n",
      "step 300, cost 263.485\n",
      "step 300, change in cost0.00387573\n",
      "step 310, training accuracy 0.201681\n",
      "step 310, cost 263.478\n",
      "step 310, change in cost0.00650024\n",
      "step 320, training accuracy 0.201681\n",
      "step 320, cost 263.465\n",
      "step 320, change in cost0.0129395\n",
      "step 330, training accuracy 0.201681\n",
      "step 330, cost 263.429\n",
      "step 330, change in cost0.0358276\n",
      "step 340, training accuracy 0.201681\n",
      "step 340, cost 263.257\n",
      "step 340, change in cost0.172852\n",
      "step 350, training accuracy 0.201681\n",
      "step 350, cost 263.042\n",
      "step 350, change in cost0.214508\n",
      "step 360, training accuracy 0.201681\n",
      "step 360, cost 263.017\n",
      "step 360, change in cost0.0255432\n",
      "step 370, training accuracy 0.201681\n",
      "step 370, cost 263.01\n",
      "step 370, change in cost0.00646973\n",
      "step 380, training accuracy 0.201681\n",
      "step 380, cost 263.007\n",
      "step 380, change in cost0.00280762\n",
      "step 390, training accuracy 0.201681\n",
      "step 390, cost 263.006\n",
      "step 390, change in cost0.00158691\n",
      "step 400, training accuracy 0.201681\n",
      "step 400, cost 263.005\n",
      "step 400, change in cost0.0010376\n",
      "step 410, training accuracy 0.201681\n",
      "step 410, cost 263.004\n",
      "step 410, change in cost0.000732422\n",
      "step 420, training accuracy 0.201681\n",
      "step 420, cost 263.003\n",
      "step 420, change in cost0.000579834\n",
      "step 430, training accuracy 0.201681\n",
      "step 430, cost 263.003\n",
      "step 430, change in cost0.000427246\n",
      "step 440, training accuracy 0.201681\n",
      "step 440, cost 263.003\n",
      "step 440, change in cost0.000366211\n",
      "step 450, training accuracy 0.201681\n",
      "step 450, cost 263.002\n",
      "step 450, change in cost0.000335693\n",
      "step 460, training accuracy 0.201681\n",
      "step 460, cost 263.002\n",
      "step 460, change in cost0.000305176\n",
      "step 470, training accuracy 0.201681\n",
      "step 470, cost 263.002\n",
      "step 470, change in cost0.000274658\n",
      "step 480, training accuracy 0.201681\n",
      "step 480, cost 263.001\n",
      "step 480, change in cost0.000213623\n",
      "step 490, training accuracy 0.201681\n",
      "step 490, cost 263.001\n",
      "step 490, change in cost0.000213623\n",
      "step 500, training accuracy 0.201681\n",
      "step 500, cost 263.001\n",
      "step 500, change in cost0.000244141\n",
      "step 510, training accuracy 0.201681\n",
      "step 510, cost 263.001\n",
      "step 510, change in cost0.000213623\n",
      "step 520, training accuracy 0.201681\n",
      "step 520, cost 263.001\n",
      "step 520, change in cost0.000183105\n",
      "step 530, training accuracy 0.201681\n",
      "step 530, cost 263\n",
      "step 530, change in cost0.000213623\n",
      "step 540, training accuracy 0.201681\n",
      "step 540, cost 263\n",
      "step 540, change in cost0.000244141\n",
      "step 550, training accuracy 0.201681\n",
      "step 550, cost 263\n",
      "step 550, change in cost0.000244141\n",
      "step 560, training accuracy 0.201681\n",
      "step 560, cost 263\n",
      "step 560, change in cost0.000213623\n",
      "step 570, training accuracy 0.201681\n",
      "step 570, cost 262.999\n",
      "step 570, change in cost0.000274658\n",
      "step 580, training accuracy 0.201681\n",
      "step 580, cost 262.999\n",
      "step 580, change in cost0.000244141\n",
      "step 590, training accuracy 0.201681\n",
      "step 590, cost 262.999\n",
      "step 590, change in cost0.000335693\n",
      "step 600, training accuracy 0.201681\n",
      "step 600, cost 262.998\n",
      "step 600, change in cost0.000335693\n",
      "step 610, training accuracy 0.201681\n",
      "step 610, cost 262.998\n",
      "step 610, change in cost0.000366211\n",
      "step 620, training accuracy 0.201681\n",
      "step 620, cost 262.998\n",
      "step 620, change in cost0.000427246\n",
      "step 630, training accuracy 0.201681\n",
      "step 630, cost 262.997\n",
      "step 630, change in cost0.000549316\n",
      "step 640, training accuracy 0.201681\n",
      "step 640, cost 262.996\n",
      "step 640, change in cost0.000640869\n",
      "step 650, training accuracy 0.201681\n",
      "step 650, cost 262.996\n",
      "step 650, change in cost0.000793457\n",
      "step 660, training accuracy 0.201681\n",
      "step 660, cost 262.995\n",
      "step 660, change in cost0.0010376\n",
      "step 670, training accuracy 0.201681\n",
      "step 670, cost 262.993\n",
      "step 670, change in cost0.00140381\n",
      "step 680, training accuracy 0.201681\n",
      "step 680, cost 262.991\n",
      "step 680, change in cost0.00201416\n",
      "step 690, training accuracy 0.201681\n",
      "step 690, cost 262.988\n",
      "step 690, change in cost0.00326538\n",
      "step 700, training accuracy 0.201681\n",
      "step 700, cost 262.982\n",
      "step 700, change in cost0.00585938\n",
      "step 710, training accuracy 0.201681\n",
      "step 710, cost 262.968\n",
      "step 710, change in cost0.0136719\n",
      "step 720, training accuracy 0.201681\n",
      "step 720, cost 262.913\n",
      "step 720, change in cost0.0557251\n",
      "step 730, training accuracy 0.201681\n",
      "step 730, cost 262.574\n",
      "step 730, change in cost0.338623\n",
      "step 740, training accuracy 0.201681\n",
      "step 740, cost 262.513\n",
      "step 740, change in cost0.0607605\n",
      "step 750, training accuracy 0.201681\n",
      "step 750, cost 262.502\n",
      "step 750, change in cost0.0109558\n",
      "step 760, training accuracy 0.201681\n",
      "step 760, cost 262.005\n",
      "step 760, change in cost0.49704\n",
      "step 770, training accuracy 0.201681\n",
      "step 770, cost 262.003\n",
      "step 770, change in cost0.00253296\n",
      "step 780, training accuracy 0.201681\n",
      "step 780, cost 262.002\n",
      "step 780, change in cost0.000427246\n",
      "step 790, training accuracy 0.201681\n",
      "step 790, cost 262.002\n",
      "step 790, change in cost0.000244141\n",
      "step 800, training accuracy 0.201681\n",
      "step 800, cost 262.002\n",
      "step 800, change in cost0.00012207\n",
      "step 810, training accuracy 0.201681\n",
      "step 810, cost 262.002\n",
      "step 810, change in cost0.00012207\n",
      "step 820, training accuracy 0.201681\n",
      "step 820, cost 262.002\n",
      "step 820, change in cost0.00012207\n",
      "step 830, training accuracy 0.201681\n",
      "step 830, cost 262.002\n",
      "step 830, change in cost6.10352e-05\n",
      "change in cost6.10352e-05; convergence.\n",
      "final accuracy on test set: 0.166667\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEYCAYAAAATRII7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqEHjA5EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fd94c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GRAPH LIVE UPDATING\n",
    "epoch_values = []\n",
    "accuracy_values = []\n",
    "cost_values = []\n",
    "# Turn on interactive plotting\n",
    "plt.ion()\n",
    "# Create the main, super plot\n",
    "fig = plt.figure()\n",
    "# Create two subplots on their own axes and give titles\n",
    "ax1 = plt.subplot(\"211\")\n",
    "ax1.set_title(\"TRAINING ACCURACY\", fontsize=18)\n",
    "ax2 = plt.subplot(\"212\")\n",
    "ax2.set_title(\"TRAINING COST\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "# RUN THE GRAPH\n",
    "\n",
    "# Create a tensorflow session\n",
    "sess = tf.Session()\n",
    "# Initialize all tensorflow variables\n",
    "sess.run(init_OP)\n",
    "\n",
    "# Ops for vizualization\n",
    "# argmax(activation_OP, 1) gives the label our model thought was most likely\n",
    "# argmax(yGold, 1) is the correct label\n",
    "correct_predictions_OP = tf.equal(tf.argmax(activation_OP, 1), tf.argmax(yGold, 1))\n",
    "# False is 0 and True is 1, what was our average?\n",
    "accuracy_OP = tf.reduce_mean(tf.cast(correct_predictions_OP, \"float\"))\n",
    "# Summary op for regression output\n",
    "activation_summary_OP = tf.histogram_summary(\"output\", activation_OP)\n",
    "# Summary op for accuracy\n",
    "accuracy_summary_OP = tf.scalar_summary(\"accuracy\", accuracy_OP)\n",
    "# Summary op for cost\n",
    "cost_summary_OP = tf.scalar_summary(\"cost\", cost_OP)\n",
    "# Summary ops to check how variables (W, b) are updating after each iteration\n",
    "weightSummary = tf.histogram_summary(\"weights\", weights.eval(session=sess))\n",
    "biasSummary = tf.histogram_summary(\"biases\", bias.eval(session=sess))\n",
    "# Merge all summaries\n",
    "all_summary_OPS = tf.merge_all_summaries()\n",
    "# Summary writer\n",
    "writer = tf.train.SummaryWriter(\"summary_logs\", sess.graph)\n",
    "\n",
    "# Initialize reporting variables\n",
    "cost = 0\n",
    "diff = 1\n",
    "\n",
    "# Training epochs\n",
    "for i in range(numEpochs):\n",
    "    if i > 1 and diff < .0001:\n",
    "        print(\"change in cost%g; convergence.\"%diff)\n",
    "        break\n",
    "    else:\n",
    "        # Run training step\n",
    "        step = sess.run(training_OP, feed_dict={X: trainX, yGold: trainY})\n",
    "        # Report occasional stats\n",
    "        if i % 10 == 0:\n",
    "            # Add epoch to epoch_values\n",
    "            epoch_values.append(i)\n",
    "            # Generate accuracy stats on test data\n",
    "            summary_results, train_accuracy, newCost = sess.run(\n",
    "                [all_summary_OPS, accuracy_OP, cost_OP],\n",
    "                feed_dict = {X: trainX, yGold: trainY}\n",
    "            )\n",
    "            # Add accuracy to live graphing variable\n",
    "            accuracy_values.append(train_accuracy)\n",
    "            # Add cost to live graphing variable\n",
    "            cost_values.append(newCost)\n",
    "            # Write summary stats to writer\n",
    "            writer.add_summary(summary_results, i)\n",
    "            # Re-assign values for variables\n",
    "            diff = abs(newCost - cost)\n",
    "            cost = newCost\n",
    "            \n",
    "            # generate print statements\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, cost %g\"%(i, newCost))\n",
    "            print(\"step %d, change in cost%g\"%(i, diff))\n",
    "            \n",
    "            # Plot progress to our two subplots\n",
    "            accuracyLine, = ax1.plot(epoch_values, accuracy_values)\n",
    "            costLine, = ax2.plot(epoch_values, cost_values)\n",
    "            fig.canvas.draw()\n",
    "            time.sleep(1)\n",
    "\n",
    "# How well do we perform on held-out test data?\n",
    "print(\"final accuracy on test set: %s\" %str(sess.run(accuracy_OP, feed_dict={X: testX, yGold: testY})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Trained Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Saver\n",
    "# saver = tf.train.Saver()\n",
    "# Save variables to .ckpt file\n",
    "# saver.save(sess, \"trained_variables.ckpt\")\n",
    "\n",
    "# Close tensorflow session\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
