{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samanthagarofalo/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_numpy_array(filePath, delimiter):\n",
    "    return np.genfromtxt(filePath, delimiter=delimiter, dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X is the TF matrix, Y is the binary response\n",
    "trainX = csv_to_numpy_array(\"/Users/samanthagarofalo/Documents/Data Science/Capstone/DSI-Religion-2017/NeuralNet/trainX.csv\", delimiter=\",\")\n",
    "trainY = csv_to_numpy_array(\"/Users/samanthagarofalo/Documents/Data Science/Capstone/DSI-Religion-2017/NeuralNet/trainY.csv\", delimiter=\",\")\n",
    "testX = csv_to_numpy_array(\"/Users/samanthagarofalo/Documents/Data Science/Capstone/DSI-Religion-2017/NeuralNet/testX.csv\", delimiter=\",\")\n",
    "testY = csv_to_numpy_array(\"/Users/samanthagarofalo/Documents/Data Science/Capstone/DSI-Religion-2017/NeuralNet/testY.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the index column from dataset\n",
    "trainX = trainX[:,1:11933]\n",
    "trainY = trainY[:,1:11933]\n",
    "testX = testX[:,1:11933]\n",
    "testY = testY[:,1:11933]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove column headers\n",
    "trainX = trainX[1:11933]\n",
    "trainY = trainY[1:11933]\n",
    "testX = testX[1:11933]\n",
    "testY = testY[1:11933]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA SET PARAMETERS\n",
    "# get dimensions for different variables and placeholders:\n",
    "# numFeatures = the number of words extracted from each article\n",
    "numFeatures = trainX.shape[1]\n",
    "# numLabels = number of classes we're predicting (here it's 4 - Bahai, DorothyDay, IntegralYoga, and SeaShepherds)\n",
    "numLabels = trainY.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING SESSION PARAMETERS\n",
    "# number of times we iterate through training data\n",
    "# tensorboard shows that accuracy plateaus at ~25k epochs\n",
    "numEpochs = 27000\n",
    "# a smarter learning rate for gradientOptimizer\n",
    "learningRate = tf.train.exponential_decay(learning_rate = 0.0008,\n",
    "                                          global_step =1,\n",
    "                                          decay_steps = trainX.shape[0],\n",
    "                                          decay_rate = 0.95,\n",
    "                                          staircase = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = X-matrix/feature-matrix/data-matrix...It's a tensor to hold the article data. 'None' here \n",
    "# means that we can hold any number of articles\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "# yGold = Y-matrix/label-matrix/labels...This will be our correct answers matrix. Every row has\n",
    "# is binary, with 1 representing their group. 'None' here means that we can hold any number of emails\n",
    "yGold = tf.placeholder(tf.float32, [None, numLabels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Values are randomly sampled from a Gaussian with a standard deviation of:\n",
    "# sqrt(6/(numInputNodes + numOutputNodes + 1))\n",
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                        mean = 0,\n",
    "                                        stddev = (np.sqrt(6/numFeatures + numLabels + 1)),\n",
    "                                        name = \"weights\"))\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                     mean = 0,\n",
    "                                     stddev = (np.sqrt(6/numFeatures + numLabels + 1)),\n",
    "                                     name = \"bias\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### PREDICTION OPS ###\n",
    "######################\n",
    "# INITIALIZE our weights and biases\n",
    "init_OP = tf.initialize_all_variables()\n",
    "# PREDICTION ALGORITHM  i.e. FEEDFORWARD ALGORITHM\n",
    "apply_weights_OP = tf.matmul(X, weights, name = \"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name = \"add_bias\")\n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name = \"activation\") # sigmoid = activation function; output = 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### EVALUATION OP ###\n",
    "#####################\n",
    "# COST FUNCTION i.e. MEAN SQUARED ERROR\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-yGold, name = \"squared_error_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### OPTIMIZATION OP ###\n",
    "#######################\n",
    "# OPTIMIZATION ALGORITHM i.e. GRADIENT DESCENT\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Graph and Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.161765\n",
      "step 0, cost 130.703\n",
      "step 0, change in cost130.703\n",
      "step 10, training accuracy 0.161765\n",
      "step 10, cost 129.89\n",
      "step 10, change in cost0.813461\n",
      "step 20, training accuracy 0.161765\n",
      "step 20, cost 129.082\n",
      "step 20, change in cost0.808304\n",
      "step 30, training accuracy 0.161765\n",
      "step 30, cost 128.273\n",
      "step 30, change in cost0.808578\n",
      "step 40, training accuracy 0.161765\n",
      "step 40, cost 127.458\n",
      "step 40, change in cost0.814796\n",
      "step 50, training accuracy 0.161765\n",
      "step 50, cost 126.631\n",
      "step 50, change in cost0.827072\n",
      "step 60, training accuracy 0.161765\n",
      "step 60, cost 125.785\n",
      "step 60, change in cost0.845749\n",
      "step 70, training accuracy 0.161765\n",
      "step 70, cost 124.914\n",
      "step 70, change in cost0.871117\n",
      "step 80, training accuracy 0.161765\n",
      "step 80, cost 124.011\n",
      "step 80, change in cost0.903481\n"
     ]
    }
   ],
   "source": [
    "# GRAPH LIVE UPDATING\n",
    "epoch_values = []\n",
    "accuracy_values = []\n",
    "cost_values = []\n",
    "# Turn on interactive plotting\n",
    "plt.ion()\n",
    "# Create the main, super plot\n",
    "fig = plt.figure()\n",
    "# Create two subplots on their own axes and give titles\n",
    "ax1 = plt.subplot(\"211\")\n",
    "ax1.set_title(\"TRAINING ACCURACY\", fontsize=18)\n",
    "ax2 = plt.subplot(\"212\")\n",
    "ax2.set_title(\"TRAINING COST\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "# RUN THE GRAPH\n",
    "\n",
    "# Create a tensorflow session\n",
    "sess = tf.Session()\n",
    "# Initialize all tensorflow variables\n",
    "sess.run(init_OP)\n",
    "\n",
    "# Ops for vizualization\n",
    "# argmax(activation_OP, 1) gives the label our model thought was most likely\n",
    "# argmax(yGold, 1) is the correct label\n",
    "correct_predictions_OP = tf.equal(tf.argmax(activation_OP, 1), tf.argmax(yGold, 1))\n",
    "# False is 0 and True is 1, what was our average?\n",
    "accuracy_OP = tf.reduce_mean(tf.cast(correct_predictions_OP, \"float\"))\n",
    "# Summary op for regression output\n",
    "activation_summary_OP = tf.histogram_summary(\"output\", activation_OP)\n",
    "# Summary op for accuracy\n",
    "accuracy_summary_OP = tf.scalar_summary(\"accuracy\", accuracy_OP)\n",
    "# Summary op for cost\n",
    "cost_summary_OP = tf.scalar_summary(\"cost\", cost_OP)\n",
    "# Summary ops to check how variables (W, b) are updating after each iteration\n",
    "weightSummary = tf.histogram_summary(\"weights\", weights.eval(session=sess))\n",
    "biasSummary = tf.histogram_summary(\"biases\", bias.eval(session=sess))\n",
    "# Merge all summaries\n",
    "all_summary_OPS = tf.merge_all_summaries()\n",
    "# Summary writer\n",
    "writer = tf.train.SummaryWriter(\"summary_logs\", sess.graph)\n",
    "\n",
    "# Initialize reporting variables\n",
    "cost = 0\n",
    "diff = 1\n",
    "\n",
    "# Training epochs\n",
    "for i in range(numEpochs):\n",
    "    if i > 1 and diff < .0001:\n",
    "        print(\"change in cost%g; convergence.\"%diff)\n",
    "        break\n",
    "    else:\n",
    "        # Run training step\n",
    "        step = sess.run(training_OP, feed_dict={X: trainX, yGold: trainY})\n",
    "        # Report occasional stats\n",
    "        if i % 10 == 0:\n",
    "            # Add epoch to epoch_values\n",
    "            epoch_values.append(i)\n",
    "            # Generate accuracy stats on test data\n",
    "            summary_results, train_accuracy, newCost = sess.run(\n",
    "                [all_summary_OPS, accuracy_OP, cost_OP],\n",
    "                feed_dict = {X: trainX, yGold: trainY}\n",
    "            )\n",
    "            # Add accuracy to live graphing variable\n",
    "            accuracy_values.append(train_accuracy)\n",
    "            # Add cost to live graphing variable\n",
    "            cost_values.append(newCost)\n",
    "            # Write summary stats to writer\n",
    "            writer.add_summary(summary_results, i)\n",
    "            # Re-assign values for variables\n",
    "            diff = abs(newCost - cost)\n",
    "            cost = newCost\n",
    "            \n",
    "            # generate print statements\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, cost %g\"%(i, newCost))\n",
    "            print(\"step %d, change in cost%g\"%(i, diff))\n",
    "            \n",
    "            # Plot progress to our two subplots\n",
    "            accuracyLine, = ax1.plot(epoch_values, accuracy_values)\n",
    "            costLine, = ax2.plot(epoch_values, cost_values)\n",
    "            fig.canvas.draw()\n",
    "            time.sleep(1)\n",
    "\n",
    "# How well do we perform on held-out test data?\n",
    "print(\"final accuracy on test set: %s\" %str(sess.run(accuracy_OP, feed_dict={X: testX, yGold: testY})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Trained Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Saver\n",
    "# saver = tf.train.Saver()\n",
    "# Save variables to .ckpt file\n",
    "# saver.save(sess, \"trained_variables.ckpt\")\n",
    "\n",
    "# Close tensorflow session\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
