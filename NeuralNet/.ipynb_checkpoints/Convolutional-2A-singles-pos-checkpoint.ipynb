{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lingualTF as la\n",
    "\n",
    "import tensorflow as tf\n",
    "#import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# REPLACE THESE WITH CORRECT PATHS #\n",
    "####################################\n",
    "docsFolder = '/Users/Seth/Documents/DSI/Capstone/DSI-Religion-2017/data_dsicap_single/'\n",
    "docRanks = pd.read_csv('/Users/Seth/Documents/DSI/Capstone/DSI-Religion-2017/refData/docRanks.csv')\n",
    "#docRanks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5002"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab from this list http://www.wordfrequency.info/5k_lemmas_download.asp\n",
    "w5k = pd.read_table('refData/5000-words.txt')\n",
    "\n",
    "## process words to match lingualObject processing\n",
    "w5k['Word'] = la.cleanTokens(w5k['Word'])\n",
    "\n",
    "vocab = list(w5k['Word']) ##### USE ONLY THE TOP 1000 (we could try more later)\n",
    "\n",
    "# add UNKNOWN and VALUE tokens\n",
    "vocab += ['UNK', 'VAL']\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'c', 'e', 'd', 'i', 'j', 'm', 'n', 'p', 'r', 'u', 't', 'v', 'x']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parts of Speech ##### Thought about using only POS + UNK + VAL, could be an option...\n",
    "pos = list(set(w5k['POS']))\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docToNumpy(doc, vocab, keywordCount=20, lengthThreshold=500, warnOnPad=False):\n",
    "    # tokenize, etc. document\n",
    "    self = la.lingualObject(doc)\n",
    "    fileName = self.fileList[0]\n",
    "    # set the keywords\n",
    "    self.setKeywords(wordCount=keywordCount)\n",
    "    # create array of each word in doc with len(vocab)-dimensional sparse one-hot vectors\n",
    "    sparse = [la.wordToInd(word, vocab, self.keywords) for word in self.tokens[fileName]]\n",
    "    # if less words than the threshold, pad with UNK\n",
    "    if len(sparse) < lengthThreshold:\n",
    "        if warnOnPad==True:\n",
    "            print('PADDING: ' + doc + ' has only ' + str(len(sparse)) + ' words. Adding ' + str((lengthThreshold - len(sparse))) + ' UNKs.')\n",
    "        sparse += [(len(vocab) - 2)] * (lengthThreshold - len(sparse))\n",
    "    # create dense array of zeros, then fill in the correct ones\n",
    "    docArr = np.zeros((lengthThreshold,len(vocab)), dtype=np.int) # default is 500, 1002\n",
    "    for i in range(0, lengthThreshold):\n",
    "        docArr[i][sparse[i]] = 1\n",
    "    # \n",
    "    return docArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docToNumpyPOS(doc, vocab, keywordCount=20, lengthThreshold=500, warnOnPad=False):\n",
    "    # tokenize, etc. document\n",
    "    self = la.lingualObject(doc)\n",
    "    fileName = self.fileList[0]\n",
    "    # set the keywords\n",
    "    self.setKeywords(wordCount=keywordCount)\n",
    "    # create array of each word in doc with len(vocab)-dimensional sparse one-hot vectors\n",
    "    sparse = [la.wordToInd(word, vocab, self.keywords) for word in self.tokens[fileName]]\n",
    "    # if less words than the threshold, pad with UNK\n",
    "    if len(sparse) < lengthThreshold:\n",
    "        if warnOnPad==True:\n",
    "            print('PADDING: ' + doc + ' has only ' + str(len(sparse)) + ' words. Adding ' + str((lengthThreshold - len(sparse))) + ' UNKs.')\n",
    "        sparse += [(len(vocab) - 2)] * (lengthThreshold - len(sparse))\n",
    "    # create dense array of zeros, then fill in the correct ones\n",
    "    docArr = np.zeros((lengthThreshold,len(vocab)), dtype=np.int) # default is 500, 1002\n",
    "    for i in range(0, lengthThreshold):\n",
    "        docArr[i][sparse[i]] = 1\n",
    "    # \n",
    "    return docArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adoc = 'ACLU03'\n",
    "doc = docsFolder + adoc + '/raw/' + adoc + '.txt'\n",
    "keywordCount=20\n",
    "lengthThreshold=500\n",
    "\n",
    "# tokenize, etc. document\n",
    "self = la.lingualObject(doc)\n",
    "fileName = self.fileList[0]\n",
    "# set the keywords\n",
    "self.setKeywords(wordCount=keywordCount)\n",
    "# create array of each word in doc with len(vocab)-dimensional sparse one-hot vectors\n",
    "sparse = [la.wordToInd(word, vocab, self.keywords) for word in self.tokens[fileName]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5000, 4, 124, 5001, 5001, 0, 143, 567, 203, 176]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten = sparse[:10]\n",
    "ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000    NaN\n",
       "4         a\n",
       "124       n\n",
       "5001    NaN\n",
       "5001    NaN\n",
       "0         a\n",
       "143       r\n",
       "567       j\n",
       "203       n\n",
       "176       v\n",
       "Name: POS, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w5k.ix[ten,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data (set up X and Y as numpy arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [01:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(193, 500, 1002)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct X matrix FROM SCRATCH\n",
    "#X = np.array([docToNumpy(docsFolder + adoc + '/raw/' + adoc + '.txt', vocab) for adoc in tqdm(docRanks['groupName'])])\n",
    "#np.save('/Users/Seth/Documents/DSI/Capstone/big-data/X-single-1k.npy', X)\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 500, 1002)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR load X matrix from disk (already processed)\n",
    "X = np.load('/Users/Seth/Documents/DSI/Capstone/big-data/X-single-5k.npy')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct Y matrix\n",
    "Y = np.array([np.array([y]) for y in docRanks['rank']])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffle the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 72,   4,  79, 171, 112,  71,  90,  93, 182, 147,\n",
       "            ...\n",
       "             96,  57, 123, 106,  83,  17,  98,  66, 126, 109],\n",
       "           dtype='int64', length=193)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the seed if you want to\n",
    "np.random.seed(123)\n",
    "#\n",
    "shuf = docRanks.sample(frac=1).index\n",
    "shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitPoint = int(len(shuf) * .8)\n",
    "#train = docRanks.iloc[shuf[:splitPoint]]\n",
    "#test = docRanks.iloc[shuf[(splitPoint+1):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[shuf[:splitPoint]]\n",
    "Y_train = Y[shuf[:splitPoint]]\n",
    "\n",
    "X_test = X[shuf[(splitPoint+1):]]\n",
    "Y_test = Y[shuf[(splitPoint+1):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## convolution1d first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution1D(64, 5, border_mode='same', input_shape=(X.shape[1],X.shape[2]), activation='relu'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same', activation='relu'))\n",
    "model.add(AveragePooling1D(pool_length=2))\n",
    "model.add(Convolution1D(32, 3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "#\n",
    "#model.add(Dense(9, activation='softmax')) # for categorical_cross_entropy, below (classification)\n",
    "model.add(Dense(1)) # for mse, below (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DSIacc(y_true, y_pred):\n",
    "    return float(len([i for i in range(len(y_pred)) if abs(y_true[i][0]-y_pred[i][0])<=1])/float(len(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.compile(loss='categorical_cross_entropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "154/154 [==============================] - 4s - loss: 13.4771 - mean_absolute_error: 3.0416     \n",
      "Epoch 2/10\n",
      "154/154 [==============================] - 4s - loss: 4.7297 - mean_absolute_error: 1.7409     \n",
      "Epoch 3/10\n",
      "154/154 [==============================] - 5s - loss: 4.5321 - mean_absolute_error: 1.7216     \n",
      "Epoch 4/10\n",
      "154/154 [==============================] - 4s - loss: 4.4898 - mean_absolute_error: 1.7050     \n",
      "Epoch 5/10\n",
      "154/154 [==============================] - 4s - loss: 4.1779 - mean_absolute_error: 1.6983     \n",
      "Epoch 6/10\n",
      "154/154 [==============================] - 4s - loss: 4.2149 - mean_absolute_error: 1.6514     \n",
      "Epoch 7/10\n",
      "154/154 [==============================] - 4s - loss: 4.6778 - mean_absolute_error: 1.7551     \n",
      "Epoch 8/10\n",
      " 32/154 [=====>........................] - ETA: 3s - loss: 4.1459 - mean_absolute_error: 1.6435"
     ]
    }
   ],
   "source": [
    "#model.fit(data, labels, nb_epoch=10, batch_size=32) ### generic call from documentation\n",
    "model.fit(X_train, Y_train, nb_epoch=10, batch_size=32)\n",
    "#model.fit(X_train, Y_train, nb_epoch=10, batch_size= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test, batch_size=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DSIacc(Y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [visLang2]",
   "language": "python",
   "name": "Python [visLang2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
