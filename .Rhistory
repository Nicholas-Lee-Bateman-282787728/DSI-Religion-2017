weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
inspect(Dtdm3)
Dtdm3 <- TermDocumentMatrix(Dc,
control = list(bounds = list(global = c(length(Dc)*.6, Inf)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
inspect(Dtdm3)
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE , stemming=TRUE this threw a weird error
))
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE , stemming=TRUE #this threw a weird error
))
data("crude")termFreq(crude[[14]])strsplit_space_tokenizer <- function(x)    unlist(strsplit(as.character(x), "[[:space:]]+"))ctrl <- list(tokenize = strsplit_space_tokenizer,             removePunctuation = list(preserve_intra_word_dashes = TRUE),             stopwords = c("reuter", "that"),             stemming = TRUE,             wordLengths = c(4, Inf))termFreq(crude[[14]], control = ctrl)
data("crude")
termFreq(crude[[14]])
strsplit_space_tokenizer <- function(x)
unlist(strsplit(as.character(x), "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = c("reuter", "that"),
stemming = TRUE,
wordLengths = c(4, Inf))
termFreq(crude[[14]], control = ctrl)
install.packages("SnowballC")
data("crude")
termFreq(crude[[14]])
strsplit_space_tokenizer <- function(x)
unlist(strsplit(as.character(x), "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = c("reuter", "that"),
stemming = TRUE,
wordLengths = c(4, Inf))
termFreq(crude[[14]], control = ctrl)
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE , stemming=TRUE #this threw a weird error
))
View(inspect(Dtdm))
library(tm)
getSources()
?XMLSource
?URISource
getwd()
setwd("/Users/Seth/Documents/Silverchair/A1 taxonomyproject/Capstone simulation")
memory.limit()
2^31-1
2^30
2^31
?strsplit
library(rvest)
args <- commandArgs(TRUE)
findWikiTerms <- function(term) {
#get wiki terms
terms <- unlist(strsplit(term, ","))
terms <- gsub(" ", "_", terms)
wikiTerms <- character(length(terms))
for (j in 1:length(terms)) {
url <- paste("https://en.wikipedia.org/wiki/", terms[j], sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
if(grepl("s$", terms[j])) {
url <- paste("https://en.wikipedia.org/wiki/", substr(terms[j], 1, nchar(terms[j]) - 1), sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
} else {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
}
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
}
winners <- paste(wikiTerms, collapse=",")
print(winners)
}
findWikiTerms(args[1])
term <- "CCD"
terms <- unlist(strsplit(term, ","))
terms
wikiTerms <- character(length(terms))
for (j in 1:length(terms)) {
url <- paste("https://en.wikipedia.org/wiki/", terms[j], sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
if(grepl("s$", terms[j])) {
url <- paste("https://en.wikipedia.org/wiki/", substr(terms[j], 1, nchar(terms[j]) - 1), sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
} else {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
}
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
}
wikiTerms
url
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
title
term <- "Airborne reconnaissance,CCD image sensors,Charge-coupled devices,Imaging systems,Line scan sensors"
findWikiTerms <- function(term) {
#get wiki terms
terms <- unlist(strsplit(term, ","))
terms <- gsub(" ", "_", terms)
wikiTerms <- character(length(terms))
for (j in 1:length(terms)) {
url <- paste("https://en.wikipedia.org/wiki/", terms[j], sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
if(grepl("s$", terms[j])) {
url <- paste("https://en.wikipedia.org/wiki/", substr(terms[j], 1, nchar(terms[j]) - 1), sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
} else {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
}
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
}
winners <- paste(wikiTerms, collapse=",")
print(winners)
}
findWikiTerms(term)
term
sin(1)
sin(0)
sin(90)
sin(1)
sin(0.1)
x <- seq(-1, 1, by=0.1)
x
plot(sin(x))
big <- (-90, 90)
big <- seq(-90, 90)
plot(sin(big))
x <- seq(-10, 10, by=0.1)
plot(sin(x))
plot(x, sin(x))
plot(x, sin(x), xlim=c(-2,3))
plot(x, sin(x), xlim=c(-5,5))
sin(3.14)
pi
sin(pi)
sin(-pi)
cos(pi)
plot(x, cos(x))
plot(x, sin(x), xlim=c(-5,5))
lines(x, cos(x))
cos(0)
sin(0)
cos(pi)
cos(2pi)
cos(2 * pi)
sin(90)
cos(90)
sin(pi/4)
sin(3*pi/4)
pig <- seq(-10,10) * pi / 4
pig
plot(sin(pig),seq(-10,10))
plot(seq(-10,10),sin(pig))
plot(seq(-10,10),sin(pig), type-"l")
plot(seq(-10,10),sin(pig), type="l")
plot(seq(-10,10),sin(pig))
sin(9*pi/4)
sin(11*pi/4)
sin(pi/2)
sin(2*pi)
shiny::runApp('Documents/DSI/Wiki Terms app/shiny2WC')
?exists
runApp('Documents/DSI/Wiki Terms app/shiny2WC')
?wordcloud
runApp('Documents/DSI/Wiki Terms app/shiny2WC')
1/1/60 - 1533
as.date(1/1/60) - 1533
vec1 <- c(1,2,3,4,5)
vec2 <- c(6,2,7,4,8)
vec1 * vec2
vec1 <- c(1,2,3,4,5)
vec2 <- c(6,2,7,4,8)
cosDist <- function(vec1, vec2) {
sum( vec1 * vec2 ) / ( sqrt(sum( vec1 * vec1 )) * sqrt(sum( vec2 * vec2 )) )
}
cosDist(vec1, vec2)
sum( vec1 * vec2 )
sqrt(sum( vec1 * vec1 ))
sqrt(sum( vec2 * vec2 ))
vec1 <- c(1,2,3,4,5)
vec2 <- c(6,2,7,4,8)
vec3 <- c(10,20,7,30,40)
vec4 <- seq(100,500, by=100)
cosDist(vec2, vec3)
cosDist(vec3, vec4)
cosDist(vec4, vec4)
cosDist(vec1, vec4)
vec4
vec5 <- c(100,200,300,400,600)
cosDist(vec4, vec5)
cosDist(vec1, vec5)
vec1r <- runif(5)
vec2r <- runif(5)
cosDist(vec1, vec1r)
cosDist(vec1r, vec2r)
vec3r <- runif(5, min=100, max=1000)
cosDist(vec1r, vec3r)
vec3r
vec1r <- runif(5)
vec2r <- runif(5)
vec3r <- runif(5, min=100, max=1000)
cosDist(vec1r, vec3r) # I totally don't get this
cville <- 239957324.07 - 179215.46
cvilleWithLawn <- 239957324.07
Lawn <- 179215.46
cvilleWithLawn - cville
lawn / cville
Lawn / cville
cville
library(randomForest)
?randomForest
library(randomForest)
# Classify the famous iris dataset:  http://en.wikipedia.org/wiki/Sepal#mediaviewer/File:Petal-sepal.jpg
data(iris)
help(iris)
summary(iris)
# Create randomized training/testing sets (75%/25%)
set.seed(1230984)
training.indices = sample(1:nrow(iris), as.integer(nrow(iris) * 0.75))
training.set = iris[training.indices,]
testing.set = iris[-training.indices,]
# Fit random forest
rf.fit <- randomForest(Species ~ ., data = training.set)
predictions = predict(rf.fit, newdata = testing.set)
# Output raw accuracy.
sum(predictions == testing.set[,"Species"]) / nrow(testing.set)
# We can also get probabilities.
predict(rf.fit, newdata = testing.set, type = "prob")
help(randomForest)
setwd("~/Documents/DSI/Capstone/DSI-Religion-2017")
library(ggplot2)
library(gridExtra)
pn10 <- plotify(read.csv('modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv'))
plotify <- function(df) {
for (i in 1:nrow(df)) {
df$groupName[i] <- unlist(strsplit(df$groupId[i], "_"))[1]
}
df <- merge(df, ranks, by = "groupName")
df$rankDiscrete <- as.factor(df$groupRank)
}
#### with pronoun judgements
pn10 <- plotify(read.csv('modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv'))
df <- read.csv('modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv')
View(df)
### PLOTIFY THE DATAFRAME
plotify <- function(df) {
df$groupId <- as.character(df$groupId)
for (i in 1:nrow(df)) {
df$groupName[i] <- unlist(strsplit(df$groupId[i], "_"))[1]
}
df <- merge(df, ranks, by = "groupName")
df$rankDiscrete <- as.factor(df$groupRank)
}
#### with pronoun judgements
pn10 <- plotify(read.csv('modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv'))
ranks <- data.frame(groupName=c('WBC', 'PastorAnderson', 'NaumanKhan', 'DorothyDay', 'JohnPiper', 'Shepherd',
'Rabbinic', 'Unitarian', 'MehrBaba','SeaShepherds',
'IntegralYoga','Bahai','ISIS'),
groupRank=c(1,2,3,4,4,4,6,7,8,2,7,6,1))
### PLOTIFY THE DATAFRAME
plotify <- function(df) {
df$groupId <- as.character(df$groupId)
for (i in 1:nrow(df)) {
df$groupName[i] <- unlist(strsplit(df$groupId[i], "_"))[1]
}
df <- merge(df, ranks, by = "groupName")
df$rankDiscrete <- as.factor(df$groupRank)
}
#### with pronoun judgements
pn10 <- plotify(read.csv('modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv'))
View(pn10)
View(df)
str(df)
df <- read.csv('modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv')
str(Df)
str(df)
plotify <- function(df) {
# df$groupId <- as.character(df$groupId)
# for (i in 1:nrow(df)) {
#   df$groupName[i] <- unlist(strsplit(df$groupId[i], "_"))[1]
# }
df <- merge(df, ranks, by = "groupName")
df$rankDiscrete <- as.factor(df$groupRank)
}
#### with pronoun judgements
pn10 <- plotify(read.csv('modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv'))
View(pn10)
View(ranks)
str(ranks)
df <- read.csv('modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv')
df <- merge(df, ranks, by = "groupName")
str(df)
View(df)
df <- read.csv('modelOutput/signalOutput-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv')
str(df)
df$groupId <- as.character(df$groupId)
for (i in 1:nrow(df)) {
df$groupName[i] <- unlist(strsplit(df$groupId[i], "_"))[1]
}
str(df)
df <- merge(df, ranks, by = "groupName")
str(df)
df$rankDiscrete <- as.factor(df$groupRank)
str(df)
### PLOTIFY THE DATAFRAME
plotify <- function(df) {
df$groupId <- as.character(df$groupId)
for (i in 1:nrow(df)) {
df$groupName[i] <- unlist(strsplit(df$groupId[i], "_"))[1]
}
df <- merge(df, ranks, by = "groupName")
df$rankDiscrete <- as.factor(df$groupRank)
}
#### with pronoun judgements
pn10 <- plotify(read.csv('modelOutput/signalOutput-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv'))
View(pn10)
### PLOTIFY THE DATAFRAME
plotify <- function(df) {
df$groupId <- as.character(df$groupId)
for (i in 1:nrow(df)) {
df$groupName[i] <- unlist(strsplit(df$groupId[i], "_"))[1]
}
df <- merge(df, ranks, by = "groupName")
df$rankDiscrete <- as.factor(df$groupRank)
df
}
#### with pronoun judgements
pn10 <- plotify(read.csv('modelOutput/signalOutput-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv'))
View(pn10)
aaDF <- plotify(read.csv('./pythonOutput/run1/cleanedOutput/coco_3_cv_3_netAng_30_sc_0/run0/masterOutput.csv', stringsAsFactors = F))
View(aaDF)
pn10 <- plotify(read.csv('modelOutput/signalOutput-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv'))
pn20 <- plotify(read.csv('modelOutput/signalOutput-coco_3_cv_3_netAng_30_twc_20_tfidfNoPro_pronoun_bin_10-PCZKXX.csv'))
g1 <- ggplot(pn10, aes(x=avgSD, y =avgEVC, colour = groupName)) + geom_point() + ggtitle("AdjAdv") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
ggplot(idfDF, aes(x=avgSD, y =avgEVC, colour = groupName)) + geom_point() + ggtitle("10") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
g2 <- ggplot(pn20, aes(x=avgSD, y =avgEVC, colour = groupName)) + geom_point() + ggtitle("20") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
grid.arrange(g1,g2,ncol=2)
g1 <- ggplot(pn10, aes(x=avgSD, y =avgEVC, colour = groupName)) + geom_point() + ggtitle("10 keywords") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
g2 <- ggplot(pn20, aes(x=avgSD, y =avgEVC, colour = groupName)) + geom_point() + ggtitle("20 keywords") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
grid.arrange(g1,g2,ncol=2)
g1 <- ggplot(pn10, aes(x=avgSD, y =judgementFrac, colour = groupName)) + geom_point() + ggtitle("10 keywords") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
g2 <- ggplot(pn20, aes(x=avgSD, y =judgementFrac, colour = groupName)) + geom_point() + ggtitle("20 keywords") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
grid.arrange(g1,g2,ncol=2)
g1 <- ggplot(pn10, aes(x=avgSD, y =avgEVC, colour = groupName)) + geom_point() + ggtitle("10 keywords") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
g2 <- ggplot(pn20, aes(x=avgSD, y =avgEVC, colour = groupName)) + geom_point() + ggtitle("20 keywords") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
grid.arrange(g1,g2,ncol=2)
g1 <- ggplot(pn10, aes(x=avgSD, y =judgementFrac, colour = groupName)) + geom_point() + ggtitle("10 keywords") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
g2 <- ggplot(pn20, aes(x=avgSD, y =judgementFrac, colour = groupName)) + geom_point() + ggtitle("20 keywords") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
grid.arrange(g1,g2,ncol=2)
ggplot(aaDF, aes(x=avgSD, y =judgementFrac, colour = groupName)) + geom_point() + ggtitle("AdjAdv")
ggplot(idfDF, aes(x=avgSD, y =judgementFrac, colour = groupName)) + geom_point() + ggtitle("TF-IDF")
idfDF <- plotify(read.csv('./modelOutput/signalOutputcoco_3_cv_3_netAng_30_sc_0-NQRDMF.csv'))
#### NoPro TFIDF (pronouns removed)
# new groups included
npDFnew <- plotify(read.csv('./modelOutput/signalOutputcoco_3_cv_3_netAng_30_sc_0-TBXLWF.csv'))
# just old groups
npDF <- plotify(read.csv('./modelOutput/signalOutputcoco_3_cv_3_netAng_30_sc_0-159PNO.csv'))
ggplot(idfDF, aes(x=avgSD, y =judgementFrac, colour = groupName)) + geom_point() + ggtitle("TF-IDF")
ggplot(aaDF, aes(x=avgSD, y =judgementFrac, colour = rankDiscrete)) + geom_point() + ggtitle("AdjAdv")
mid = 4
## zoomed in
g1 <- ggplot(pn10, aes(x=avgSD, y =avgEVC, colour = groupRank)) + geom_point() + scale_color_gradient2(midpoint=mid, low="red", mid="white", high="blue", space ="Lab" ) + ggtitle("10") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
g2 <- ggplot(pn20, aes(x=avgSD, y =avgEVC, colour = groupRank)) + geom_point() + scale_color_gradient2(midpoint=mid, low="red", mid="white", high="blue", space ="Lab" ) + ggtitle("20") + theme(legend.position="none") + xlim(.54,.85) + ylim(.38,.93)
grid.arrange(g1,g2,ncol=2)
g1 <- ggplot(aaDF, aes(x=perPos, y =perNeg, colour = groupRank)) + geom_point() + scale_color_gradient2(midpoint=mid, low="red", mid="white", high="blue", space ="Lab" ) + ggtitle("Sentiment") + theme(legend.position="none")
g1 <- ggplot(pn10, aes(x=perPos, y =perNeg, colour = groupRank)) + geom_point() + scale_color_gradient2(midpoint=mid, low="red", mid="white", high="blue", space ="Lab" ) + ggtitle("Sentiment") + theme(legend.position="none")
#PROBLEM!!! #ggplot(idfDF, aes(x=perPos, y =perNeg, colour = groupRank)) + geom_point() + scale_color_gradient2(midpoint=mid, low="red", mid="white", high="blue", space ="Lab" ) + ggtitle("TF-IDF")
## Judgements by RANK  ######DIFFERENCE!!!!
g2 <- ggplot(pn10, aes(x=judgementCount, y =judgementFrac, colour = groupRank)) + geom_point() + scale_color_gradient2(midpoint=mid, low="red", mid="white", high="blue", space ="Lab" ) + ggtitle("Judgements") + theme(legend.position="none")
#ggplot(idfDF, aes(x=judgementCount, y =judgementFrac, colour = groupRank)) + geom_point() + scale_color_gradient2(midpoint=mid, low="red", mid="white", high="blue", space ="Lab" ) + ggtitle("TF-IDF")
grid.arrange(g1,g2,ncol=2)
ggplot(pn10, aes(x=judgementFrac, y =judgementCount, colour = groupName)) + geom_point() + ggtitle("AdjAdv")
ggplot(pn10, aes(x=judgementFrac, y =judgementCount, colour = groupName)) + geom_point() + ggtitle("10")
ggplot(pn20, aes(x=judgementFrac, y =judgementCount, colour = groupName)) + geom_point() + ggtitle("20")
##########
accuracy <- function(df, returnDF = F) {
RFscores <- logical(nrow(df))
SVMscores <- logical(nrow(df))
for (i in 1:nrow(df)) {
#RFscores[i] <- df$rank[i] >= (df$rfPred[i] - .5) & df$rank[i] <= (df$rfPred[i] + .5)
#SVMscores[i] <- df$rank[i] >= (df$svmPred[i] - .5) & df$rank[i] <= (df$svmPred[i] + .5)
RFscores[i] <- abs(df$rank[i] - df$rfPred[i]) <= 1
SVMscores[i] <- abs(df$rank[i] - df$svmPred[i]) <= 1
}
scores <- c((sum(RFscores)/length(RFscores)), (sum(SVMscores)/length(SVMscores)))
# return data frame
if (returnDF == T) {
print(scores)
data.frame(df$groupId, RFscores, SVMscores)
} else {
print(scores)
}
}
# new groups included
pn10 <- read.csv('./modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_10_tfidfNoPro_pronoun_bin_10-9VXA7R.csv', stringsAsFactors = F)
# just old groups
pn20 <- read.csv('./modelOutput/modelPredictions-coco_3_cv_3_netAng_30_twc_20_tfidfNoPro_pronoun_bin_10-PCZKXX.csv', stringsAsFactors = F)
for (i in 1:nrow(pn10)) {
pn10$groupName[i] <- unlist(strsplit(pn10$groupId[i], "_"))[1]
}
for (i in 1:nrow(pn20)) {
pn20$groupName[i] <- unlist(strsplit(pn20$groupId[i], "_"))[1]
}
accuracy(pn10)
accuracy(pn20)
View(pn10)
setwd("~/Documents/DSI/Capstone/DSI-Religion-2017")
library(ggplot2)
library(gridExtra)
##########
accuracy <- function(df, returnDF = F) {
RFscores <- logical(nrow(df))
SVMscores <- logical(nrow(df))
for (i in 1:nrow(df)) {
#RFscores[i] <- df$rank[i] >= (df$rfPred[i] - .5) & df$rank[i] <= (df$rfPred[i] + .5)
#SVMscores[i] <- df$rank[i] >= (df$svmPred[i] - .5) & df$rank[i] <= (df$svmPred[i] + .5)
RFscores[i] <- abs(df$rank[i] - df$rfPred[i]) <= 1
SVMscores[i] <- abs(df$rank[i] - df$svmPred[i]) <= 1
}
scores <- c((sum(RFscores)/length(RFscores)), (sum(SVMscores)/length(SVMscores)))
# return data frame
if (returnDF == T) {
print(scores)
data.frame(df$groupId, RFscores, SVMscores)
} else {
print(scores)
}
}
#### COMPARE NEW TFIDF METHODS
idfDF <- read.csv('./modelOutput/modelPredictions-coco_3_cv_3_netAng_30_sc_0-NQRDMF.csv', stringsAsFactors = F)
for (i in 1:nrow(idfDF)) {
idfDF$groupName[i] <- unlist(strsplit(idfDF$groupId[i], "_"))[1]
}
#### NoPro TFIDF (pronouns removed)
# new groups included
npDFnew <- read.csv('./modelOutput/modelPredictions-coco_3_cv_3_netAng_30_sc_0-TBXLWF.csv', stringsAsFactors = F)
# just old groups
npDF <- read.csv('./modelOutput/modelPredictions-coco_3_cv_3_netAng_30_sc_0-159PNO.csv', stringsAsFactors = F)
for (i in 1:nrow(npDF)) {
npDF$groupName[i] <- unlist(strsplit(npDF$groupId[i], "_"))[1]
}
for (i in 1:nrow(npDFnew)) {
npDFnew$groupName[i] <- unlist(strsplit(npDFnew$groupId[i], "_"))[1]
}
accuracy(idfDF)
accuracy(npDF) # good!
accuracy(npDFnew)
