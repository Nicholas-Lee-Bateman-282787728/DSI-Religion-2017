pertaining to processes such as protein–protein interactions
and protein–disease associations from the literature.
Automatic event extraction has a broad range of applications
in systems biology, ranging from support for the
creation and annotation of pathways to automatic population
or enrichment of databases. Event extraction systems
can be trained to recognize a wide range of activities,
including protein–protein interactions, pathway enrichment
and construction, gene regulatory events, and metabolic
or signaling reactions. The purpose of this review is to
describe the resources and techniques involved in developing
tools for event extraction from the literature for
systems biology applications. To develop event extraction
systems, annotated corpora are needed. In this review, we
examine the features of a number of event annotation
corpora used by TM systems for training and development.
We also examine a number of approaches to event extraction,
ranging in sophistication from pattern matching to
systems that include rich linguistic features based on full
parsing.",
'Michael Jeffrey Jordan (born February 17, 1963), also known by his initials, MJ,[3] is an American former professional basketball player. He is also a businessman, and principal owner and chairman of the Charlotte Hornets. Jordan played 15 seasons in the National Basketball Association (NBA) for the Chicago Bulls and Washington Wizards. His biography on the NBA website states: "By acclamation, Michael Jordan is the greatest basketball player of all time."[4] Jordan was one of the most effectively marketed athletes of his generation and was considered instrumental in popularizing the NBA around the world in the 1980s and 1990s.[5] After a three-season stint playing for coach Dean Smith at the University of North Carolina, where he was a member of the Tar Heels\' national championship team in 1982, Jordan joined the NBA\'s Chicago Bulls in 1984. He quickly emerged as a league star, entertaining crowds with his prolific scoring. His leaping ability, illustrated by performing slam dunks from the free throw line in slam dunk contests, earned him the nicknames "Air Jordan" and "His Airness". He also gained a reputation for being one of the best defensive players in basketball.[6] In 1991, he won his first NBA championship with the Bulls, and followed that achievement with titles in 1992 and 1993, securing a "three-peat". Although Jordan abruptly retired from basketball before the beginning of the 1993–94 NBA season to pursue a career in baseball, he returned to the Bulls in March 1995 and led them to three additional championships in 1996, 1997, and 1998, as well as an NBA-record 72 regular-season wins in the 1995–96 NBA season. Jordan retired for a second time in January 1999, but returned for two more NBA seasons from 2001 to 2003 as a member of the Wizards.'
)
####
splitem <- function(sentence) {
sentence <- gsub("\\n", " ", sentence)
sentence <- tolower(gsub("[^A-Za-z0-9'# -]", "", sentence)) #what we're keeping
words <- unlist(strsplit(sentence, "[ \n]")) #actually make the split
# data frame of term counts
Di <- data.frame(table(words))
Di
}
#DO WE WANT NGRAMS? IF SO USE THIS ONE
splitemNG <- function(sentence) {
sentence <- gsub("\\n", " ", sentence)
sentence <- tolower(gsub("[^A-Za-z0-9'# -]", "", sentence)) #what we're keeping
words <- unlist(strsplit(sentence, "[ \n]")) #actually make the split
twograms <- make.ngrams(words, ngram.size=2) #make 2-grams
# data frame of term counts
DFwords <- data.frame(table(words))
DFtg <- data.frame(table(twograms))
names(DFwords) <- c("term", "freq")
names(DFtg) <- c("term", "freq")
#combine into master data frame
Di <- rbind(DFwords, DFtg)
Di
}
TFIDFperword <- function(tj, Di, Corpus) {
Di <- splitem(Di)
M = length(Corpus) # the number of documents in the Corpus
nij = Di[Di[,1]==tj,2] # the number of times tj appears in Di
ni1k = sum(Di[,2]) # the number of words in Di
itoj = length(grep(tj, Corpus, ignore.case=T)) #the number of documents in our corpus that contain tj
wji = (nij/ni1k) * log(M/itoj)
wji
}
# try to think of a more efficient way than loading the whole corpus in
TFIDFperword("algorithm", DRaw[1], DRaw)
TFIDF <- function(Di, Corpus) {
Di <- splitem(Di)
Di$tfidf <- numeric(nrow(Di))
M = length(Corpus) # the number of documents in the Corpus
ni1k = sum(Di[,2]) # the number of words in Di
for (j in 1:nrow(Di)) {
tj = as.character(Di[j,1])
nij = Di[Di[,1]==tj,2] # the number of times tj appears in Di
itoj = length(grep(tj, Corpus, ignore.case=T)) #the number of documents in our corpus that contain tj
Di$tfidf[j] =(nij/ni1k) * log(M/itoj)
}
Di <- Di[order(Di$tfidf, decreasing=T),]
Di <- Di[Di$tfidf>0 & Di$tfidf<1,] ## NEED TO DEAL WITH THE INFINITES
Di
}
D <- list()
for (i in 1:length(DRaw)) {
D[[i]] <- TFIDF(DRaw[i], DRaw)
}
Dc <- VCorpus(VectorSource(DRaw))
Dc
Dc[[2]]
meta(Dc[[2]])
Dcsw <- tm_map(Dc, removeWords, stopwords("english"))
as.character(Dcsw[[2]])
Dcst <- tm_map(Dcsw, stemDocument)
as.character(Dcst[[2]])
tm_map(Dcsw, stemDocument)
as.character(Dcsw[[2]])
Dtdm <- TermDocumentMatrix(Dc)
View(Dtdm)
inspect(Ddtm[5:10, 740:743])
inspect(Dtdm[5:10, 740:743])
inspect(Dtdm[5:10, 40:43])
Dtdm
inspect(Dtdm[5:10, ])
inspect(Dtdm[5:30, ])
inspect(Dtdm[105:130, ])
View(inspect(Dtdm[105:130, ]))
naw <- inspect(Dtdm[105:130, ])
View(naw)
Dc
length(Dc)
Dtdm
df <- data.frame(Dtdm)
df <- data.frame(inspect(Dtdm))
View(df)
length(Dc*.2)
length(Dc)*.2)
length(Dc)*.2
data("crude")tdm <- TermDocumentMatrix(crude,                          control = list(removePunctuation = TRUE,                                         stopwords = TRUE))
tdm <- TermDocumentMatrix(Dc,
control = list(removePunctuation = TRUE,
stopwords = TRUE))
inspect(tdm[1:25,])
tdmnaw <- TermDocumentMatrix(Dc)
inspect(tdmnaw[1:25,])
inspect(tdmnaw[Terms=="and",])
inspect(tdmnaw[tdmnaw$Terms=="and",])
inspect(tdmnaw[tdmnaw$Terms=="[5].",])
View(inspect(tdmnaw))
View(inspect(tdm))
tdm <- TermDocumentMatrix(Dc,
control = list(removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
stemming=TRUE))
View(inspect(tdm))
tdm <- TermDocumentMatrix(Dc,
control = list(removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
stemming=TRUE))
tdm <- TermDocumentMatrix(Dc,
control = list(removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE))
View(tdm)
View(inspect(tdm))
Dtdm <- TermDocumentMatrix(Dc,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
View(inspect(Dtdm))
inspect(tdm[c("basketball", "algorithm", "text"), ])
inspect(Dtdm[c("basketball", "algorithm", "text"), ])
Dtdmb <- TermDocumentMatrix(Dc,
control = list(bounds = list(global = c(1, length(Dc)*.2)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
Dtdmb <- TermDocumentMatrix(Dc,
control = list(bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
inspect(Dtdmb[c("basketball", "algorithm", "text"), ])
inspect(Dtdmb[c("basketball", "algorithm"), ])
View(inspect(Dtdmb))
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
Dtdm3 <- TermDocumentMatrix(Dc,
control = list(bounds = list(global = c(length(Dc)*.5, Inf)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
inspect(Dtdm3)
Dtdm3 <- TermDocumentMatrix(Dc,
control = list(bounds = list(global = c(length(Dc)*.6, Inf)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
inspect(Dtdm3)
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE #, stemming=TRUE this threw a weird error
))
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE , stemming=TRUE this threw a weird error
))
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE , stemming=TRUE #this threw a weird error
))
data("crude")termFreq(crude[[14]])strsplit_space_tokenizer <- function(x)    unlist(strsplit(as.character(x), "[[:space:]]+"))ctrl <- list(tokenize = strsplit_space_tokenizer,             removePunctuation = list(preserve_intra_word_dashes = TRUE),             stopwords = c("reuter", "that"),             stemming = TRUE,             wordLengths = c(4, Inf))termFreq(crude[[14]], control = ctrl)
data("crude")
termFreq(crude[[14]])
strsplit_space_tokenizer <- function(x)
unlist(strsplit(as.character(x), "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = c("reuter", "that"),
stemming = TRUE,
wordLengths = c(4, Inf))
termFreq(crude[[14]], control = ctrl)
install.packages("SnowballC")
data("crude")
termFreq(crude[[14]])
strsplit_space_tokenizer <- function(x)
unlist(strsplit(as.character(x), "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer,
removePunctuation = list(preserve_intra_word_dashes = TRUE),
stopwords = c("reuter", "that"),
stemming = TRUE,
wordLengths = c(4, Inf))
termFreq(crude[[14]], control = ctrl)
Dtdm <- TermDocumentMatrix(Dc,
control = list(#bounds = list(global = c(1, length(Dc)*.5)), # so that it only consider terms that appear in less than 20% of the documents
weighting =
function(x)
weightTfIdf(x, normalize =
TRUE),
removePunctuation = TRUE,
removeNumbers = TRUE,
stopwords = TRUE , stemming=TRUE #this threw a weird error
))
View(inspect(Dtdm))
library(tm)
getSources()
?XMLSource
?URISource
getwd()
setwd("/Users/Seth/Documents/Silverchair/A1 taxonomyproject/Capstone simulation")
memory.limit()
2^31-1
2^30
2^31
?strsplit
library(rvest)
args <- commandArgs(TRUE)
findWikiTerms <- function(term) {
#get wiki terms
terms <- unlist(strsplit(term, ","))
terms <- gsub(" ", "_", terms)
wikiTerms <- character(length(terms))
for (j in 1:length(terms)) {
url <- paste("https://en.wikipedia.org/wiki/", terms[j], sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
if(grepl("s$", terms[j])) {
url <- paste("https://en.wikipedia.org/wiki/", substr(terms[j], 1, nchar(terms[j]) - 1), sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
} else {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
}
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
}
winners <- paste(wikiTerms, collapse=",")
print(winners)
}
findWikiTerms(args[1])
term <- "CCD"
terms <- unlist(strsplit(term, ","))
terms
wikiTerms <- character(length(terms))
for (j in 1:length(terms)) {
url <- paste("https://en.wikipedia.org/wiki/", terms[j], sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
if(grepl("s$", terms[j])) {
url <- paste("https://en.wikipedia.org/wiki/", substr(terms[j], 1, nchar(terms[j]) - 1), sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
} else {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
}
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
}
wikiTerms
url
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
title
term <- "Airborne reconnaissance,CCD image sensors,Charge-coupled devices,Imaging systems,Line scan sensors"
findWikiTerms <- function(term) {
#get wiki terms
terms <- unlist(strsplit(term, ","))
terms <- gsub(" ", "_", terms)
wikiTerms <- character(length(terms))
for (j in 1:length(terms)) {
url <- paste("https://en.wikipedia.org/wiki/", terms[j], sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
if(grepl("s$", terms[j])) {
url <- paste("https://en.wikipedia.org/wiki/", substr(terms[j], 1, nchar(terms[j]) - 1), sep="")
title <- try(url %>% read_html %>% html_nodes(xpath='/html/head/title') %>% html_text, silent=T)
if(class(title)=="try-error") {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
} else {
wikiTerms[j] <- paste(terms[j], "_NOWIKI", sep="")
}
} else {
wikiTerms[j] <- unlist(strsplit(title, " - "))[1]
}
}
winners <- paste(wikiTerms, collapse=",")
print(winners)
}
findWikiTerms(term)
term
sin(1)
sin(0)
sin(90)
sin(1)
sin(0.1)
x <- seq(-1, 1, by=0.1)
x
plot(sin(x))
big <- (-90, 90)
big <- seq(-90, 90)
plot(sin(big))
x <- seq(-10, 10, by=0.1)
plot(sin(x))
plot(x, sin(x))
plot(x, sin(x), xlim=c(-2,3))
plot(x, sin(x), xlim=c(-5,5))
sin(3.14)
pi
sin(pi)
sin(-pi)
cos(pi)
plot(x, cos(x))
plot(x, sin(x), xlim=c(-5,5))
lines(x, cos(x))
cos(0)
sin(0)
cos(pi)
cos(2pi)
cos(2 * pi)
sin(90)
cos(90)
sin(pi/4)
sin(3*pi/4)
pig <- seq(-10,10) * pi / 4
pig
plot(sin(pig),seq(-10,10))
plot(seq(-10,10),sin(pig))
plot(seq(-10,10),sin(pig), type-"l")
plot(seq(-10,10),sin(pig), type="l")
plot(seq(-10,10),sin(pig))
sin(9*pi/4)
sin(11*pi/4)
sin(pi/2)
sin(2*pi)
shiny::runApp('Documents/DSI/Wiki Terms app/shiny2WC')
?exists
runApp('Documents/DSI/Wiki Terms app/shiny2WC')
?wordcloud
runApp('Documents/DSI/Wiki Terms app/shiny2WC')
1/1/60 - 1533
as.date(1/1/60) - 1533
vec1 <- c(1,2,3,4,5)
vec2 <- c(6,2,7,4,8)
vec1 * vec2
vec1 <- c(1,2,3,4,5)
vec2 <- c(6,2,7,4,8)
cosDist <- function(vec1, vec2) {
sum( vec1 * vec2 ) / ( sqrt(sum( vec1 * vec1 )) * sqrt(sum( vec2 * vec2 )) )
}
cosDist(vec1, vec2)
sum( vec1 * vec2 )
sqrt(sum( vec1 * vec1 ))
sqrt(sum( vec2 * vec2 ))
vec1 <- c(1,2,3,4,5)
vec2 <- c(6,2,7,4,8)
vec3 <- c(10,20,7,30,40)
vec4 <- seq(100,500, by=100)
cosDist(vec2, vec3)
cosDist(vec3, vec4)
cosDist(vec4, vec4)
cosDist(vec1, vec4)
vec4
vec5 <- c(100,200,300,400,600)
cosDist(vec4, vec5)
cosDist(vec1, vec5)
vec1r <- runif(5)
vec2r <- runif(5)
cosDist(vec1, vec1r)
cosDist(vec1r, vec2r)
vec3r <- runif(5, min=100, max=1000)
cosDist(vec1r, vec3r)
vec3r
vec1r <- runif(5)
vec2r <- runif(5)
vec3r <- runif(5, min=100, max=1000)
cosDist(vec1r, vec3r) # I totally don't get this
cville <- 239957324.07 - 179215.46
cvilleWithLawn <- 239957324.07
Lawn <- 179215.46
cvilleWithLawn - cville
lawn / cville
Lawn / cville
cville
setwd("~/Documents/DSI/Capstone/DSI-Religion-2017")
signalDF <- read.csv('/pythonOutput/run1/cleanedOutput/coco_3_cv_3_netAng_30_sc_0/run0/masterOutput.csv')
signalDF <- read.csv('pythonOutput/run1/cleanedOutput/coco_3_cv_3_netAng_30_sc_0/run0/masterOutput.csv')
signalDF <- read.csv('./pythonOutput/run1/cleanedOutput/coco_3_cv_3_netAng_30_sc_0/run0/masterOutput.csv')
View(signalDF)
library(ggplot2)
names(signalDF)
ggplot(signalDF, aes(x=avgSD, y =avgEVC)) + geom_point()
ggplot(signalDF, aes(x=avgSD, y =avgEVC, colours = groupId)) + geom_point()
str(signalDF)
ggplot(signalDF, aes(x=avgSD, y =avgEVC, colour = groupId)) + geom_point()
unlist(strsplit(signalDF$groupId, "_"))[1]
signalDF <- read.csv('./pythonOutput/run1/cleanedOutput/coco_3_cv_3_netAng_30_sc_0/run0/masterOutput.csv', stringsAsFactors = F)
unlist(strsplit(signalDF$groupId, "_"))[1]
unlist(strsplit(signalDF$groupId, "_"))
signalDF$groupName <- unlist(strsplit(signalDF$groupId, "_"))[1]
View(signalDF)
ggplot(signalDF, aes(x=avgSD, y =avgEVC, colour = groupName)) + geom_point()
for (i in 1:nrow(signalDF)) {
signalDF$groupName[i] <- unlist(strsplit(signalDF$groupId[i], "_"))[1]
}
table(signalDF$groupName)
ggplot(signalDF, aes(x=avgSD, y =avgEVC, colour = groupName)) + geom_point()
names(signalDF)
ggplot(signalDF, aes(x=avgSD, y =judgementFrac, colour = groupName)) + geom_point()
groupNameList=c('WBC', 'PastorAnderson', 'NaumanKhan', 'DorothyDay', 'JohnPiper', 'Shepherd',
'Rabbinic', 'Unitarian', 'MehrBaba','NawDawg','SeaShepherds','IntegralYoga','Bahai')
groupRankList=c(1,2,3,4,4,4,6,7,8,4,2,7,6)
ranks <- data.frame(groupNameList=c('WBC', 'PastorAnderson', 'NaumanKhan', 'DorothyDay', 'JohnPiper', 'Shepherd',
'Rabbinic', 'Unitarian', 'MehrBaba','NawDawg','SeaShepherds','IntegralYoga','Bahai'),
groupRankList=c(1,2,3,4,4,4,6,7,8,4,2,7,6))
View(ranks)
?merge
DF <- merge(signalDF, ranks)
View(DF)
ranks <- data.frame(groupName=c('WBC', 'PastorAnderson', 'NaumanKhan', 'DorothyDay', 'JohnPiper', 'Shepherd',
'Rabbinic', 'Unitarian', 'MehrBaba','NawDawg','SeaShepherds','IntegralYoga','Bahai'),
groupRankList=c(1,2,3,4,4,4,6,7,8,4,2,7,6))
DF <- merge(signalDF, ranks, by = groupName)
ranks <- data.frame(groupName=c('WBC', 'PastorAnderson', 'NaumanKhan', 'DorothyDay', 'JohnPiper', 'Shepherd',
'Rabbinic', 'Unitarian', 'MehrBaba','NawDawg','SeaShepherds','IntegralYoga','Bahai'),
groupRankList=c(1,2,3,4,4,4,6,7,8,4,2,7,6))
DF <- merge(signalDF, ranks, by = "groupName")
View(DF)
ranks <- data.frame(groupName=c('WBC', 'PastorAnderson', 'NaumanKhan', 'DorothyDay', 'JohnPiper', 'Shepherd',
'Rabbinic', 'Unitarian', 'MehrBaba','NawDawg','SeaShepherds','IntegralYoga','Bahai'),
groupRank=c(1,2,3,4,4,4,6,7,8,4,2,7,6))
DF <- merge(signalDF, ranks, by = "groupName")
ggplot(DF, aes(x=avgSD, y =judgementFrac, colour = groupRank)) + geom_point()
DF$rankDiscrete <- as.factor(DF$groupRank)
ggplot(DF, aes(x=avgSD, y =judgementFrac, colour = rankDiscrete)) + geom_point()
ggplot(DF, aes(x=avgSD, y =judgementFrac, colour = groupRank)) + geom_point()
## RANKINGS discrete
DF$rankDiscrete <- as.factor(DF$groupRank)
ggplot(DF, aes(x=avgSD, y =judgementFrac, colour = rankDiscrete)) + geom_point()
