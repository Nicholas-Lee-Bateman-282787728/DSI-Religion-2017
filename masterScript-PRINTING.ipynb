{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Seth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /Users/Seth/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Seth/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'./refData/positive-words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bf8bc9f50024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msemanticDensity\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msyntacticParsing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msentimentAnalysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkQuantification\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Seth/Documents/DSI/Capstone/2016-group/cloneOf2016Code/prototype_python/sentimentAnalysis.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnegFilePath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./refData/negative-words.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msentDict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposFilePath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnegFilePath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mposWords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposFilePath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mnegWords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnegFilePath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: './refData/positive-words.txt'"
     ]
    }
   ],
   "source": [
    "# %load masterScript.py\n",
    "\"\"\"\n",
    "Created on Thu Jun  2 15:23:11 2016\n",
    "\n",
    "@author: nmvenuti\n",
    "\"\"\"\n",
    "import time\n",
    "start=time.time()\n",
    "import sys, os\n",
    "import gc\n",
    "#os.chdir('./github/nmvenuti/DSI_Religion/')\n",
    "#from joblib import Parallel, delayed\n",
    "#import multiprocessing as mp\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "sys.path.append('./prototype_python/')\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import semanticDensity as sd\n",
    "import syntacticParsing as sp\n",
    "import sentimentAnalysis as sa\n",
    "import networkQuantification as nq\n",
    "\n",
    "end=time.time()\n",
    "#sys.stdout = open(\"output.txt\", \"a\")\n",
    "print(str(datetime.now()))\n",
    "print('finished loading packages after '+str(end-start)+' seconds')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "#####Define Functions#####\n",
    "##########################\n",
    "def textAnalysis(paramList):\n",
    "    startTime=time.time()\n",
    "    groupId=paramList[0]\n",
    "    fileList=paramList[1]\n",
    "    targetWordCount=paramList[2]\n",
    "    cocoWindow=paramList[3]\n",
    "    svdInt=paramList[4]\n",
    "    cvWindow=paramList[5]\n",
    "    simCount=paramList[6]\n",
    "    startCount=paramList[7]\n",
    "    netAngle=paramList[8]    \n",
    "    \n",
    "    #Get list of subfiles\n",
    "    subFileList=[x[1] for x in fileList if x[0]==groupId[0] and x[2]==groupId[1]]\n",
    "    \n",
    "    tokenList = sd.tokenize(subFileList)\n",
    "    \n",
    "    ########################\n",
    "    ###Sentiment Analysis###\n",
    "    ########################\n",
    "    sentimentList=sa.sentimentLookup(tokenList)\n",
    "    \n",
    "    ########################################\n",
    "    ###POS Tagging and Judgement Analysis###\n",
    "    ########################################\n",
    "    \n",
    "    judgementList=[sp.judgements(sp.readText(fileName)) for fileName in subFileList]\n",
    "    judgementAvg=list(np.mean(np.array(judgementList),axis=0))\n",
    "    \n",
    "    txtString=' '.join([sp.readText(fileName) for fileName in subFileList])\n",
    "    wordList=sp.targetWords(txtString,targetWordCount,startCount)\n",
    "    \n",
    "    #######################            \n",
    "    ###Semantic analysis###\n",
    "    #######################\n",
    "    \n",
    "    #Get word coCo\n",
    "    CoCo, TF, docTF = sd.coOccurence(tokenList,cocoWindow)\n",
    "    \n",
    "    #Get DSM\n",
    "    DSM=sd.DSM(CoCo,svdInt)\n",
    "    \n",
    "    #Get context vectors\n",
    "    #Bring in wordlist\n",
    "    \n",
    "    wordList=[stemmer.stem(word) for word in wordList]\n",
    "    CVDict=sd.contextVectors(tokenList, DSM, wordList, cvWindow)\n",
    "    \n",
    "    #Run cosine sim\n",
    "    cosineSimilarity=sd.averageCosine(CVDict,subFileList,wordList,simCount)\n",
    "    avgSD=np.mean([x[1] for x in cosineSimilarity])\n",
    "    \n",
    "    ############################\n",
    "    ###Network Quantification###\n",
    "    ############################\n",
    "    avgEVC=nq.getNetworkQuant(DSM,wordList,netAngle)\n",
    "    \n",
    "    endTime=time.time()\n",
    "    timeRun=endTime-startTime\n",
    "    print('finished running'+'_'.join(groupId)+' in '+str(end-start)+' seconds')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    #Delete and garbage collect\n",
    "    del CoCo, TF, docTF, DSM, CVDict, cosineSimilarity\n",
    "    gc.collect()\n",
    "    #Append outputs to masterOutput\n",
    "    return(['_'.join(groupId)]+[len(subFileList),timeRun]+sentimentList+judgementAvg+[avgSD]+[avgEVC])   \n",
    "\n",
    "def runMaster(rawPath,groupList,crossValidate,groupSize,targetWordCount,startCount,cocoWindow,svdInt,cvWindow,netAngle,simCount):\n",
    "    ###############################\n",
    "    #####Raw File List Extract#####\n",
    "    ###############################\n",
    "\n",
    "    rawFileList=[]\n",
    "    for groupId in groupList:\n",
    "        for dirpath, dirnames, filenames in os.walk(rawPath+groupId+'/raw'):\n",
    "            for filename in [f for f in filenames ]:\n",
    "                if '.txt' in filename:\n",
    "                    rawFileList.append([groupId,os.path.join(dirpath, filename)])\n",
    "\n",
    "    #Make output directory\n",
    "#    runDirectory='./pythonOutput/'+ time.strftime(\"%c\")\n",
    "#    runDirectory='./pythonOutput/cocowindow_'+str(cocoWindow)+time.strftime(\"%c\").replace(' ','_')\n",
    "    runDirectory='./pythonOutput/coco_'+str(cocoWindow)+'_cv_'+str(cvWindow)+'_netAng_'+str(netAngle)+'_sc_'+str(startCount)\n",
    "    os.makedirs(runDirectory)\n",
    "    end=time.time()\n",
    "    print('finished loading packages after '+str(end-start)+' seconds')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    #Perform analysis for each fold in cross validation\n",
    "    for fold in range(crossValidate):                \n",
    "        ###############################                \n",
    "        #####Set up random binning#####\n",
    "        ###############################\n",
    "                        \n",
    "        fileDF=pd.read_csv('./data_dsicap/test_train/fileSplit_'+str(fold)+'.csv')\n",
    "        \n",
    "        fileList=fileDF.values.tolist()\n",
    "        fileList=[[fileList[i][1],fileList[i][2],fileList[i][3]] for i in range(len(fileList))]\n",
    "        \n",
    "        \n",
    "        #Get set of subgroups\n",
    "        subgroupList=[ list(y) for y in set((x[0],x[2]) for x in fileList) ]\n",
    "        \n",
    "        #Make output directory\n",
    "        outputDirectory=runDirectory+'/run'+str(fold)\n",
    "        os.makedirs(outputDirectory)\n",
    "        \n",
    "        #Print file splits to runDirectory\n",
    "        fileDF.to_csv(outputDirectory+'/fileSplits.csv')\n",
    "\n",
    "        end=time.time()\n",
    "        print('finished randomly creating subgroups '+str(end-start)+' seconds')\n",
    "        sys.stdout.flush()        \n",
    "        \n",
    "        ################################\n",
    "        #####Perform group analysis#####\n",
    "        ################################\n",
    "        \n",
    "        #Create paramList\n",
    "        paramList=[[x,fileList,targetWordCount,cocoWindow,svdInt,cvWindow,simCount,startCount,netAngle] for x in subgroupList]\n",
    "        \n",
    "        #Run calculation\n",
    "#        xPool=mp.Pool(processes=nCores)    \n",
    "#        outputList=[xPool.apply_async(textAnalysis, args=(x,)) for x in paramList]\n",
    "#        masterOutput=[p.get() for p in outputList]    \n",
    "        masterOutput=[textAnalysis(x) for x in paramList]\n",
    "#        masterOutput=textAnalysis(paramList[0])  \n",
    "#        masterOutput=[masterOutput,masterOutput]\n",
    "#        masterOutput=Parallel(n_jobs=2)(delayed(textAnalysis)(x) for x in paramList)  \n",
    "        #Create output file\n",
    "        outputDF=pd.DataFrame(masterOutput,columns=['groupId','files','timeRun','perPos','perNeg','perPosDoc','perNegDoc','judgementCount','judgementFrac','avgSD','avgEVC'])\n",
    "        outputDF.to_csv(outputDirectory+'/masterOutput.csv')\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Set inital conditions and run\n",
    "if __name__ == '__main__':\n",
    "    startTimeTotal=time.time()\n",
    "    rawPath = './data_dsicap/'\n",
    "    groupList=['DorothyDay','JohnPiper','MehrBaba','NaumanKhan','PastorAnderson',\n",
    "       'Rabbinic','Shepherd','Unitarian','WBC']\n",
    "    cocoWindow=int(sys.argv[1])\n",
    "    cvWindow=int(sys.argv[2])\n",
    "    startCount=int(sys.argv[3])\n",
    "    netAngle=int(sys.argv[4])\n",
    "#    cocoWindow=2\n",
    "#    cvWindow=2\n",
    "#    startCount=0\n",
    "#    netAngle=15\n",
    "    crossValidate=3\n",
    "    groupSize=10\n",
    "#    testSplit=.3\n",
    "    targetWordCount=10\n",
    "    svdInt=50\n",
    "    simCount=1000\n",
    "    print('cocoWindow '+str(cocoWindow))\n",
    "    sys.stdout.flush()\n",
    "    print('cvWindow '+str(cvWindow))\n",
    "    sys.stdout.flush()\n",
    "    print('netAngle '+str(netAngle))\n",
    "    sys.stdout.flush()\n",
    "    print('startCount '+str(startCount))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    runMaster(rawPath,groupList,crossValidate,groupSize,targetWordCount,startCount,cocoWindow,svdInt,cvWindow,netAngle,simCount)\n",
    "        \n",
    "    endTimeTotal=time.time()\n",
    "    print('finished entire run in :'+str((endTimeTotal-startTimeTotal)/60)+' minutes')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
